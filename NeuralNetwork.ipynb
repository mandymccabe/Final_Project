{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reading data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# for modeling\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import plotly as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example website\n",
    "https://medium.com/luca-chuangs-bapm-notes/build-a-neural-network-in-python-multi-class-classification-e940f74bd899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data\n",
    "#take input from provisional database (csv)\n",
    "# Load the csv file from GitHub\n",
    "url = 'https://raw.githubusercontent.com/mandymccabe/Final_Project/janet_branch/Data/Final_Project_Full.csv'\n",
    "url2= 'https://raw.githubusercontent.com/mandymccabe/Final_Project/main/Resources/all_responses_coded.csv'\n",
    "df = pd.read_csv(url, index_col=0)\n",
    "df2 = pd.read_csv(url2, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df2= df2.drop(['A1','A2','A3','A4','A5','A21','StartDate','EndDate'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>political_views</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respondentid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6176264298</th>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6176263960</th>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6176258621</th>\n",
       "      <td>Liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6176257082</th>\n",
       "      <td>Liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6176256111</th>\n",
       "      <td>Liberal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             political_views\n",
       "respondentid                \n",
       "6176264298          Moderate\n",
       "6176263960          Moderate\n",
       "6176258621           Liberal\n",
       "6176257082           Liberal\n",
       "6176256111           Liberal"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PoliticalViews = df.filter([\"political_views\"], axis=1)\n",
    "PoliticalViews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RespondentID</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A13</th>\n",
       "      <th>A14</th>\n",
       "      <th>...</th>\n",
       "      <th>A54</th>\n",
       "      <th>A55</th>\n",
       "      <th>A56</th>\n",
       "      <th>A57</th>\n",
       "      <th>A58</th>\n",
       "      <th>A59</th>\n",
       "      <th>A60</th>\n",
       "      <th>A61</th>\n",
       "      <th>A62</th>\n",
       "      <th>political_views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6176264298</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6176263960</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6176258621</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6176257082</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6176256111</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Liberal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RespondentID  A6  A7  A8  A9  A10  A11  A12  A13  A14  ...  A54  A55  A56  \\\n",
       "0    6176264298   0   1   0   0    1    0    1    0    0  ...    0    0    0   \n",
       "1    6176263960   0   1   0   0    1    0    0    0    0  ...    0    0    0   \n",
       "2    6176258621   0   1   0   0    1    0    0    1    0  ...    0    0    0   \n",
       "3    6176257082   0   1   0   0    0    0    0    1    0  ...    0    1    0   \n",
       "4    6176256111   0   1   0   0    1    1    1    0    0  ...    0    0    1   \n",
       "\n",
       "   A57  A58  A59  A60  A61  A62  political_views  \n",
       "0    1    0    0    0    0    0         Moderate  \n",
       "1    0    0    0    1    0    0         Moderate  \n",
       "2    0    0    0    1    0    0          Liberal  \n",
       "3    1    0    0    0    0    0          Liberal  \n",
       "4    1    0    0    0    0    0          Liberal  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dfs = pd.merge(clean_df2, PoliticalViews, how='outer', left_on=[\"RespondentID\"], right_on=['respondentid'])\n",
    "merged_dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1021, 57)\n",
      "(1021,)\n"
     ]
    }
   ],
   "source": [
    "# split into X and Y\n",
    "Y = merged_dfs['political_views']\n",
    "X = merged_dfs.drop(['political_views'], axis=1)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "# convert to numpy arrays\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Moderate\n",
       "1    Moderate\n",
       "2     Liberal\n",
       "3     Liberal\n",
       "4     Liberal\n",
       "Name: political_views, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show Y\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with labels\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 1 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_inputs= len(X[0])\n",
    "hidden_nodes_1 = 8\n",
    "hidden_nodes_2 =8\n",
    "hidden_nodes_3=8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 8)                 464       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 5)                 45        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 653\n",
      "Trainable params: 653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build a model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=hidden_nodes_1, activation='relu', input_dim= number_inputs))\n",
    "model.add(Dense(units=hidden_nodes_2, activation='relu'))\n",
    "model.add(Dense(units=hidden_nodes_3, activation='relu'))\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', # this is different instead of binary_crossentropy (for regular classification)\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# early stopping callback\n",
    "# This callback will stop the training when there is no improvement in  \n",
    "# the validation loss for 10 consecutive epochs.  \n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                   mode='min',\n",
    "                                   patience=10, \n",
    "                                   restore_best_weights=True) # important - otherwise you just return the last weigths...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "26/26 [==============================] - 1s 9ms/step - loss: 554252224.0000 - accuracy: 0.1385 - val_loss: 418016032.0000 - val_accuracy: 0.1902\n",
      "Epoch 2/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 309166048.0000 - accuracy: 0.1630 - val_loss: 228648176.0000 - val_accuracy: 0.1073\n",
      "Epoch 3/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 157752992.0000 - accuracy: 0.1691 - val_loss: 102466432.0000 - val_accuracy: 0.1073\n",
      "Epoch 4/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 52692396.0000 - accuracy: 0.1581 - val_loss: 16075115.0000 - val_accuracy: 0.1073\n",
      "Epoch 5/500\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 8596191.0000 - accuracy: 0.2880 - val_loss: 5058272.5000 - val_accuracy: 0.4293\n",
      "Epoch 6/500\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 4249935.0000 - accuracy: 0.2733 - val_loss: 4025411.5000 - val_accuracy: 0.4293\n",
      "Epoch 7/500\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 3580297.7500 - accuracy: 0.2341 - val_loss: 3212872.5000 - val_accuracy: 0.2098\n",
      "Epoch 8/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2509741.2500 - accuracy: 0.2647 - val_loss: 2554650.0000 - val_accuracy: 0.2098\n",
      "Epoch 9/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 2409274.7500 - accuracy: 0.2892 - val_loss: 2965114.0000 - val_accuracy: 0.0634\n",
      "Epoch 10/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1240732.5000 - accuracy: 0.2757 - val_loss: 1679631.5000 - val_accuracy: 0.4293\n",
      "Epoch 11/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2318705.0000 - accuracy: 0.2892 - val_loss: 1246108.2500 - val_accuracy: 0.2098\n",
      "Epoch 12/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2230688.2500 - accuracy: 0.2917 - val_loss: 2848399.5000 - val_accuracy: 0.4293\n",
      "Epoch 13/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2346646.7500 - accuracy: 0.2904 - val_loss: 3437724.0000 - val_accuracy: 0.2098\n",
      "Epoch 14/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 2798000.0000 - accuracy: 0.2721 - val_loss: 1888893.5000 - val_accuracy: 0.4293\n",
      "Epoch 15/500\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1570417.3750 - accuracy: 0.2574 - val_loss: 2830099.7500 - val_accuracy: 0.4293\n",
      "Epoch 16/500\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2091961.2500 - accuracy: 0.3051 - val_loss: 1735826.7500 - val_accuracy: 0.4293\n",
      "Epoch 17/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 2505890.2500 - accuracy: 0.2574 - val_loss: 4024075.0000 - val_accuracy: 0.4293\n",
      "Epoch 18/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 2588920.7500 - accuracy: 0.2831 - val_loss: 1661618.8750 - val_accuracy: 0.1902\n",
      "Epoch 19/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1844366.7500 - accuracy: 0.3002 - val_loss: 1975692.7500 - val_accuracy: 0.2098\n",
      "Epoch 20/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1370857.0000 - accuracy: 0.2855 - val_loss: 980017.6875 - val_accuracy: 0.4293\n",
      "Epoch 21/500\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1798683.1250 - accuracy: 0.2721 - val_loss: 1244852.8750 - val_accuracy: 0.2098\n",
      "Epoch 22/500\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1852906.8750 - accuracy: 0.2696 - val_loss: 869115.6875 - val_accuracy: 0.1902\n",
      "Epoch 23/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 3257242.0000 - accuracy: 0.2745 - val_loss: 3946762.5000 - val_accuracy: 0.0634\n",
      "Epoch 24/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2681166.0000 - accuracy: 0.3027 - val_loss: 972146.8750 - val_accuracy: 0.1902\n",
      "Epoch 25/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 2347663.7500 - accuracy: 0.2708 - val_loss: 1323445.7500 - val_accuracy: 0.2098\n",
      "Epoch 26/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 2902397.2500 - accuracy: 0.2488 - val_loss: 1129408.1250 - val_accuracy: 0.4293\n",
      "Epoch 27/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1944190.7500 - accuracy: 0.2733 - val_loss: 1726557.0000 - val_accuracy: 0.0634\n",
      "Epoch 28/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2971815.7500 - accuracy: 0.3150 - val_loss: 3372362.2500 - val_accuracy: 0.2098\n",
      "Epoch 29/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2208794.2500 - accuracy: 0.2770 - val_loss: 2891357.2500 - val_accuracy: 0.1073\n",
      "Epoch 30/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2743939.7500 - accuracy: 0.2843 - val_loss: 2328081.7500 - val_accuracy: 0.1073\n",
      "Epoch 31/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2582825.0000 - accuracy: 0.2782 - val_loss: 2391714.2500 - val_accuracy: 0.4293\n",
      "Epoch 32/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2741575.5000 - accuracy: 0.2439 - val_loss: 1732796.6250 - val_accuracy: 0.1902\n",
      "Epoch 33/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1834714.3750 - accuracy: 0.3076 - val_loss: 1369856.5000 - val_accuracy: 0.2098\n",
      "Epoch 34/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 2189595.2500 - accuracy: 0.2696 - val_loss: 3680454.2500 - val_accuracy: 0.4293\n",
      "Epoch 35/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2244626.2500 - accuracy: 0.2708 - val_loss: 1195717.1250 - val_accuracy: 0.4293\n",
      "Epoch 36/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2394275.0000 - accuracy: 0.2855 - val_loss: 3090683.2500 - val_accuracy: 0.2098\n",
      "Epoch 37/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1662795.6250 - accuracy: 0.2745 - val_loss: 2936978.0000 - val_accuracy: 0.1902\n",
      "Epoch 38/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1739166.1250 - accuracy: 0.2696 - val_loss: 1472477.6250 - val_accuracy: 0.2098\n",
      "Epoch 39/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 2755085.5000 - accuracy: 0.2402 - val_loss: 2438010.7500 - val_accuracy: 0.4293\n",
      "Epoch 40/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1732194.0000 - accuracy: 0.2659 - val_loss: 2477316.0000 - val_accuracy: 0.4293\n",
      "Epoch 41/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1659844.3750 - accuracy: 0.2647 - val_loss: 1048570.0000 - val_accuracy: 0.4293\n",
      "Epoch 42/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1934613.6250 - accuracy: 0.2917 - val_loss: 4151045.2500 - val_accuracy: 0.4293\n",
      "Epoch 43/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2672938.7500 - accuracy: 0.2855 - val_loss: 2620845.5000 - val_accuracy: 0.0634\n",
      "Epoch 44/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 2870531.2500 - accuracy: 0.2684 - val_loss: 2125951.7500 - val_accuracy: 0.4293\n",
      "Epoch 45/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1942301.0000 - accuracy: 0.2757 - val_loss: 1779555.6250 - val_accuracy: 0.0634\n",
      "Epoch 46/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 2848179.2500 - accuracy: 0.2426 - val_loss: 3397684.5000 - val_accuracy: 0.4293\n",
      "Epoch 47/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 3007393.5000 - accuracy: 0.2672 - val_loss: 2194389.5000 - val_accuracy: 0.1902\n",
      "Epoch 48/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2140623.5000 - accuracy: 0.3002 - val_loss: 1892542.2500 - val_accuracy: 0.2098\n",
      "Epoch 49/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1698368.5000 - accuracy: 0.2537 - val_loss: 1052719.5000 - val_accuracy: 0.1902\n",
      "Epoch 50/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1969749.1250 - accuracy: 0.2806 - val_loss: 794817.6875 - val_accuracy: 0.4293\n",
      "Epoch 51/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1934239.8750 - accuracy: 0.2953 - val_loss: 1183553.5000 - val_accuracy: 0.1073\n",
      "Epoch 52/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2930514.0000 - accuracy: 0.2414 - val_loss: 1779839.1250 - val_accuracy: 0.2098\n",
      "Epoch 53/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 2186756.5000 - accuracy: 0.2574 - val_loss: 1133941.2500 - val_accuracy: 0.4293\n",
      "Epoch 54/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1396188.8750 - accuracy: 0.2635 - val_loss: 1654689.1250 - val_accuracy: 0.4293\n",
      "Epoch 55/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1681052.7500 - accuracy: 0.3235 - val_loss: 1799089.1250 - val_accuracy: 0.1902\n",
      "Epoch 56/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1719035.7500 - accuracy: 0.2377 - val_loss: 1511536.7500 - val_accuracy: 0.4293\n",
      "Epoch 57/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1548250.2500 - accuracy: 0.2831 - val_loss: 2785606.5000 - val_accuracy: 0.1902\n",
      "Epoch 58/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 2856175.7500 - accuracy: 0.2819 - val_loss: 3665147.2500 - val_accuracy: 0.2098\n",
      "Epoch 59/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2241401.0000 - accuracy: 0.2586 - val_loss: 1206745.1250 - val_accuracy: 0.4293\n",
      "Epoch 60/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1786267.2500 - accuracy: 0.2757 - val_loss: 2090485.1250 - val_accuracy: 0.4293\n",
      "Epoch 61/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1902041.0000 - accuracy: 0.2782 - val_loss: 1479644.1250 - val_accuracy: 0.1073\n",
      "Epoch 62/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1525347.8750 - accuracy: 0.2770 - val_loss: 1758777.8750 - val_accuracy: 0.2098\n",
      "Epoch 63/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1763894.6250 - accuracy: 0.2672 - val_loss: 2697546.5000 - val_accuracy: 0.4293\n",
      "Epoch 64/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1697039.5000 - accuracy: 0.2537 - val_loss: 1190632.5000 - val_accuracy: 0.4293\n",
      "Epoch 65/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1576744.7500 - accuracy: 0.2794 - val_loss: 1973249.5000 - val_accuracy: 0.2098\n",
      "Epoch 66/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2215438.0000 - accuracy: 0.2745 - val_loss: 1280587.5000 - val_accuracy: 0.1902\n",
      "Epoch 67/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1992666.0000 - accuracy: 0.2708 - val_loss: 985890.8125 - val_accuracy: 0.4293\n",
      "Epoch 68/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 3118371.7500 - accuracy: 0.2782 - val_loss: 3452803.5000 - val_accuracy: 0.4293\n",
      "Epoch 69/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2174076.5000 - accuracy: 0.2659 - val_loss: 1651734.7500 - val_accuracy: 0.2098\n",
      "Epoch 70/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1208287.7500 - accuracy: 0.2684 - val_loss: 1932141.6250 - val_accuracy: 0.2098\n",
      "Epoch 71/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1713501.3750 - accuracy: 0.2451 - val_loss: 1549688.8750 - val_accuracy: 0.0634\n",
      "Epoch 72/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1339867.0000 - accuracy: 0.2623 - val_loss: 2249499.5000 - val_accuracy: 0.2098\n",
      "Epoch 73/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 2064724.5000 - accuracy: 0.2904 - val_loss: 1534017.5000 - val_accuracy: 0.2098\n",
      "Epoch 74/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1338854.8750 - accuracy: 0.2647 - val_loss: 1070965.7500 - val_accuracy: 0.4293\n",
      "Epoch 75/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1657441.0000 - accuracy: 0.2880 - val_loss: 1311307.2500 - val_accuracy: 0.4293\n",
      "Epoch 76/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1042220.6875 - accuracy: 0.2794 - val_loss: 797385.1250 - val_accuracy: 0.1902\n",
      "Epoch 77/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1374863.6250 - accuracy: 0.3174 - val_loss: 2474060.7500 - val_accuracy: 0.1902\n",
      "Epoch 78/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1954346.5000 - accuracy: 0.2549 - val_loss: 2294103.0000 - val_accuracy: 0.4293\n",
      "Epoch 79/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1632360.6250 - accuracy: 0.2941 - val_loss: 2187798.0000 - val_accuracy: 0.1902\n",
      "Epoch 80/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 2953568.2500 - accuracy: 0.2819 - val_loss: 2255138.7500 - val_accuracy: 0.2098\n",
      "Epoch 81/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1862814.3750 - accuracy: 0.2537 - val_loss: 1929108.0000 - val_accuracy: 0.2098\n",
      "Epoch 82/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1601326.1250 - accuracy: 0.2806 - val_loss: 1675470.6250 - val_accuracy: 0.2098\n",
      "Epoch 83/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1444548.8750 - accuracy: 0.2696 - val_loss: 2850998.7500 - val_accuracy: 0.1073\n",
      "Epoch 84/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1989634.5000 - accuracy: 0.2904 - val_loss: 2371764.5000 - val_accuracy: 0.1902\n",
      "Epoch 85/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1283037.1250 - accuracy: 0.2525 - val_loss: 202790.4062 - val_accuracy: 0.4293\n",
      "Epoch 86/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1540642.0000 - accuracy: 0.2868 - val_loss: 1239486.5000 - val_accuracy: 0.1902\n",
      "Epoch 87/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1798897.0000 - accuracy: 0.2855 - val_loss: 2212932.2500 - val_accuracy: 0.1073\n",
      "Epoch 88/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1679384.3750 - accuracy: 0.2672 - val_loss: 1712290.1250 - val_accuracy: 0.4293\n",
      "Epoch 89/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1451782.8750 - accuracy: 0.2770 - val_loss: 3109123.5000 - val_accuracy: 0.1902\n",
      "Epoch 90/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 2309044.7500 - accuracy: 0.2512 - val_loss: 1896803.8750 - val_accuracy: 0.1902\n",
      "Epoch 91/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 2052295.0000 - accuracy: 0.2978 - val_loss: 830792.6875 - val_accuracy: 0.1902\n",
      "Epoch 92/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1430611.1250 - accuracy: 0.2892 - val_loss: 415551.1562 - val_accuracy: 0.1902\n",
      "Epoch 93/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1355785.0000 - accuracy: 0.2500 - val_loss: 1536086.6250 - val_accuracy: 0.4293\n",
      "Epoch 94/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1463359.8750 - accuracy: 0.2904 - val_loss: 950360.5000 - val_accuracy: 0.4293\n",
      "Epoch 95/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 899345.0000 - accuracy: 0.2672 - val_loss: 1416340.0000 - val_accuracy: 0.1902\n",
      "Epoch 96/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1606470.3750 - accuracy: 0.2966 - val_loss: 1334384.1250 - val_accuracy: 0.0634\n",
      "Epoch 97/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1950457.3750 - accuracy: 0.2672 - val_loss: 2061916.7500 - val_accuracy: 0.2098\n",
      "Epoch 98/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1311605.6250 - accuracy: 0.2770 - val_loss: 648822.6875 - val_accuracy: 0.1073\n",
      "Epoch 99/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1020841.9375 - accuracy: 0.2770 - val_loss: 682337.3125 - val_accuracy: 0.4293\n",
      "Epoch 100/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1173587.7500 - accuracy: 0.2574 - val_loss: 843950.7500 - val_accuracy: 0.4293\n",
      "Epoch 101/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1144287.2500 - accuracy: 0.2770 - val_loss: 1664806.7500 - val_accuracy: 0.1073\n",
      "Epoch 102/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1562706.3750 - accuracy: 0.2549 - val_loss: 1421653.8750 - val_accuracy: 0.4293\n",
      "Epoch 103/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1253074.2500 - accuracy: 0.2806 - val_loss: 1429051.7500 - val_accuracy: 0.4293\n",
      "Epoch 104/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 981604.3750 - accuracy: 0.2819 - val_loss: 653256.1875 - val_accuracy: 0.4293\n",
      "Epoch 105/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1242024.5000 - accuracy: 0.2721 - val_loss: 1415994.5000 - val_accuracy: 0.4293\n",
      "Epoch 106/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1695833.8750 - accuracy: 0.2708 - val_loss: 934520.8750 - val_accuracy: 0.2098\n",
      "Epoch 107/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1841480.0000 - accuracy: 0.2721 - val_loss: 773193.4375 - val_accuracy: 0.4293\n",
      "Epoch 108/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1393928.3750 - accuracy: 0.2978 - val_loss: 2491705.0000 - val_accuracy: 0.1073\n",
      "Epoch 109/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1473769.3750 - accuracy: 0.2880 - val_loss: 1438366.2500 - val_accuracy: 0.1073\n",
      "Epoch 110/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1378807.3750 - accuracy: 0.2745 - val_loss: 464655.8438 - val_accuracy: 0.4293\n",
      "Epoch 111/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1215400.1250 - accuracy: 0.2770 - val_loss: 1211500.7500 - val_accuracy: 0.1073\n",
      "Epoch 112/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1496785.2500 - accuracy: 0.2586 - val_loss: 2079501.6250 - val_accuracy: 0.1902\n",
      "Epoch 113/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2427255.2500 - accuracy: 0.3125 - val_loss: 2999065.2500 - val_accuracy: 0.4293\n",
      "Epoch 114/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1919448.3750 - accuracy: 0.2610 - val_loss: 431354.9375 - val_accuracy: 0.1902\n",
      "Epoch 115/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 961008.5000 - accuracy: 0.2843 - val_loss: 1063365.2500 - val_accuracy: 0.0634\n",
      "Epoch 116/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1989161.8750 - accuracy: 0.2782 - val_loss: 1824936.1250 - val_accuracy: 0.1902\n",
      "Epoch 117/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1476405.1250 - accuracy: 0.2855 - val_loss: 1512717.3750 - val_accuracy: 0.1902\n",
      "Epoch 118/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1533931.0000 - accuracy: 0.2451 - val_loss: 2882552.2500 - val_accuracy: 0.2098\n",
      "Epoch 119/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1617571.8750 - accuracy: 0.2708 - val_loss: 1793407.7500 - val_accuracy: 0.1902\n",
      "Epoch 120/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1388779.8750 - accuracy: 0.2843 - val_loss: 912750.3125 - val_accuracy: 0.1902\n",
      "Epoch 121/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1595767.8750 - accuracy: 0.2365 - val_loss: 1025047.1875 - val_accuracy: 0.1902\n",
      "Epoch 122/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1162030.7500 - accuracy: 0.2941 - val_loss: 727593.0625 - val_accuracy: 0.2098\n",
      "Epoch 123/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1258090.0000 - accuracy: 0.2770 - val_loss: 1697145.5000 - val_accuracy: 0.2098\n",
      "Epoch 124/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1491584.0000 - accuracy: 0.2733 - val_loss: 1475521.8750 - val_accuracy: 0.1902\n",
      "Epoch 125/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 2038067.7500 - accuracy: 0.2463 - val_loss: 1140363.3750 - val_accuracy: 0.4293\n",
      "Epoch 126/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1176935.7500 - accuracy: 0.3002 - val_loss: 1724130.6250 - val_accuracy: 0.4293\n",
      "Epoch 127/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 764984.0625 - accuracy: 0.2941 - val_loss: 935391.0000 - val_accuracy: 0.1902\n",
      "Epoch 128/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 972177.5625 - accuracy: 0.2586 - val_loss: 1181749.5000 - val_accuracy: 0.1902\n",
      "Epoch 129/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 953174.2500 - accuracy: 0.2574 - val_loss: 641613.5625 - val_accuracy: 0.2098\n",
      "Epoch 130/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1317446.3750 - accuracy: 0.2819 - val_loss: 2041550.3750 - val_accuracy: 0.4293\n",
      "Epoch 131/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1566852.2500 - accuracy: 0.2819 - val_loss: 1910775.6250 - val_accuracy: 0.1902\n",
      "Epoch 132/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1190907.0000 - accuracy: 0.2770 - val_loss: 1058082.0000 - val_accuracy: 0.4293\n",
      "Epoch 133/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1645629.1250 - accuracy: 0.2745 - val_loss: 1659924.5000 - val_accuracy: 0.1073\n",
      "Epoch 134/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1607710.1250 - accuracy: 0.2745 - val_loss: 1363926.1250 - val_accuracy: 0.1073\n",
      "Epoch 135/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1266445.5000 - accuracy: 0.2794 - val_loss: 1979477.1250 - val_accuracy: 0.1902\n",
      "Epoch 136/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 893608.6875 - accuracy: 0.2684 - val_loss: 399399.0312 - val_accuracy: 0.4293\n",
      "Epoch 137/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 923473.3125 - accuracy: 0.2672 - val_loss: 899591.6250 - val_accuracy: 0.2098\n",
      "Epoch 138/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 839572.6250 - accuracy: 0.2757 - val_loss: 452953.9062 - val_accuracy: 0.4293\n",
      "Epoch 139/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1417848.7500 - accuracy: 0.2966 - val_loss: 2339666.0000 - val_accuracy: 0.1902\n",
      "Epoch 140/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1174949.2500 - accuracy: 0.2733 - val_loss: 1304952.3750 - val_accuracy: 0.1902\n",
      "Epoch 141/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1387536.3750 - accuracy: 0.2672 - val_loss: 1466649.0000 - val_accuracy: 0.2098\n",
      "Epoch 142/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1390281.1250 - accuracy: 0.2953 - val_loss: 2888958.2500 - val_accuracy: 0.2098\n",
      "Epoch 143/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2108374.2500 - accuracy: 0.2721 - val_loss: 1669916.8750 - val_accuracy: 0.4293\n",
      "Epoch 144/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1972470.8750 - accuracy: 0.2377 - val_loss: 695255.3125 - val_accuracy: 0.2098\n",
      "Epoch 145/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1017101.1875 - accuracy: 0.2623 - val_loss: 612828.8750 - val_accuracy: 0.4293\n",
      "Epoch 146/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1489533.5000 - accuracy: 0.2966 - val_loss: 418648.8125 - val_accuracy: 0.2098\n",
      "Epoch 147/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1620019.8750 - accuracy: 0.2525 - val_loss: 1694374.7500 - val_accuracy: 0.1902\n",
      "Epoch 148/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 900212.5000 - accuracy: 0.2708 - val_loss: 511096.6562 - val_accuracy: 0.1902\n",
      "Epoch 149/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1133171.8750 - accuracy: 0.2647 - val_loss: 1439427.1250 - val_accuracy: 0.1902\n",
      "Epoch 150/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 977864.2500 - accuracy: 0.2525 - val_loss: 925998.6250 - val_accuracy: 0.2098\n",
      "Epoch 151/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 946877.8125 - accuracy: 0.2733 - val_loss: 918838.3125 - val_accuracy: 0.4293\n",
      "Epoch 152/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1088652.8750 - accuracy: 0.2708 - val_loss: 1868116.7500 - val_accuracy: 0.4293\n",
      "Epoch 153/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1015023.4375 - accuracy: 0.2819 - val_loss: 717934.9375 - val_accuracy: 0.2098\n",
      "Epoch 154/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 821035.1875 - accuracy: 0.2855 - val_loss: 683869.3750 - val_accuracy: 0.4293\n",
      "Epoch 155/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1002208.3125 - accuracy: 0.2978 - val_loss: 722959.6875 - val_accuracy: 0.2098\n",
      "Epoch 156/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 643034.4375 - accuracy: 0.2574 - val_loss: 398057.2812 - val_accuracy: 0.4293\n",
      "Epoch 157/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1118064.1250 - accuracy: 0.2549 - val_loss: 1865181.3750 - val_accuracy: 0.1902\n",
      "Epoch 158/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 905360.9375 - accuracy: 0.2659 - val_loss: 413191.3438 - val_accuracy: 0.4293\n",
      "Epoch 159/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 982816.2500 - accuracy: 0.2929 - val_loss: 1261343.8750 - val_accuracy: 0.2098\n",
      "Epoch 160/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1248565.3750 - accuracy: 0.2635 - val_loss: 857020.6250 - val_accuracy: 0.1902\n",
      "Epoch 161/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 935897.2500 - accuracy: 0.2831 - val_loss: 575438.7500 - val_accuracy: 0.1902\n",
      "Epoch 162/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1031824.9375 - accuracy: 0.2316 - val_loss: 957212.3125 - val_accuracy: 0.4293\n",
      "Epoch 163/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 838065.7500 - accuracy: 0.2623 - val_loss: 1383553.5000 - val_accuracy: 0.2098\n",
      "Epoch 164/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 913758.5625 - accuracy: 0.2806 - val_loss: 910925.5625 - val_accuracy: 0.1073\n",
      "Epoch 165/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 710378.8750 - accuracy: 0.2745 - val_loss: 1015532.3125 - val_accuracy: 0.1073\n",
      "Epoch 166/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1403401.2500 - accuracy: 0.2598 - val_loss: 1509487.1250 - val_accuracy: 0.4293\n",
      "Epoch 167/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1592854.8750 - accuracy: 0.2733 - val_loss: 2547594.2500 - val_accuracy: 0.1073\n",
      "Epoch 168/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1202024.5000 - accuracy: 0.2426 - val_loss: 2458207.5000 - val_accuracy: 0.2098\n",
      "Epoch 169/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1672410.3750 - accuracy: 0.2684 - val_loss: 940081.8125 - val_accuracy: 0.4293\n",
      "Epoch 170/500\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1132295.2500 - accuracy: 0.2623 - val_loss: 2597171.5000 - val_accuracy: 0.4293\n",
      "Epoch 171/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 977764.5625 - accuracy: 0.2978 - val_loss: 579285.3125 - val_accuracy: 0.4293\n",
      "Epoch 172/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 687051.6875 - accuracy: 0.2929 - val_loss: 398321.2188 - val_accuracy: 0.2098\n",
      "Epoch 173/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1023125.7500 - accuracy: 0.2745 - val_loss: 520706.6875 - val_accuracy: 0.2098\n",
      "Epoch 174/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1390524.5000 - accuracy: 0.2794 - val_loss: 1602102.1250 - val_accuracy: 0.1902\n",
      "Epoch 175/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1141145.6250 - accuracy: 0.2549 - val_loss: 1577342.0000 - val_accuracy: 0.4293\n",
      "Epoch 176/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1672631.6250 - accuracy: 0.2537 - val_loss: 2757778.5000 - val_accuracy: 0.4293\n",
      "Epoch 177/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1559178.3750 - accuracy: 0.2708 - val_loss: 1303738.3750 - val_accuracy: 0.1902\n",
      "Epoch 178/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 897853.1875 - accuracy: 0.2843 - val_loss: 559042.2500 - val_accuracy: 0.4293\n",
      "Epoch 179/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 872346.7500 - accuracy: 0.3039 - val_loss: 1094118.0000 - val_accuracy: 0.1902\n",
      "Epoch 180/500\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 814506.8125 - accuracy: 0.2574 - val_loss: 662209.0625 - val_accuracy: 0.4293\n",
      "Epoch 181/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 897452.5625 - accuracy: 0.2574 - val_loss: 799479.1250 - val_accuracy: 0.1902\n",
      "Epoch 182/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1240515.5000 - accuracy: 0.2868 - val_loss: 1377883.7500 - val_accuracy: 0.4293\n",
      "Epoch 183/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1713278.3750 - accuracy: 0.2549 - val_loss: 1116233.1250 - val_accuracy: 0.1073\n",
      "Epoch 184/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 743349.2500 - accuracy: 0.2635 - val_loss: 372573.7500 - val_accuracy: 0.2098\n",
      "Epoch 185/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 877901.8125 - accuracy: 0.2390 - val_loss: 1258185.3750 - val_accuracy: 0.4293\n",
      "Epoch 186/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1152309.2500 - accuracy: 0.2708 - val_loss: 902906.0625 - val_accuracy: 0.1902\n",
      "Epoch 187/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1044948.6250 - accuracy: 0.2426 - val_loss: 664161.3750 - val_accuracy: 0.1902\n",
      "Epoch 188/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 978957.4375 - accuracy: 0.2831 - val_loss: 621672.0000 - val_accuracy: 0.4293\n",
      "Epoch 189/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1135806.5000 - accuracy: 0.2475 - val_loss: 694623.6250 - val_accuracy: 0.4293\n",
      "Epoch 190/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1222122.6250 - accuracy: 0.2843 - val_loss: 897906.5000 - val_accuracy: 0.2098\n",
      "Epoch 191/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1061060.3750 - accuracy: 0.2892 - val_loss: 667434.3750 - val_accuracy: 0.4293\n",
      "Epoch 192/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1218431.8750 - accuracy: 0.2574 - val_loss: 2614693.5000 - val_accuracy: 0.1073\n",
      "Epoch 193/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1622443.0000 - accuracy: 0.2684 - val_loss: 1514237.2500 - val_accuracy: 0.1902\n",
      "Epoch 194/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1349358.3750 - accuracy: 0.2304 - val_loss: 1506814.2500 - val_accuracy: 0.4293\n",
      "Epoch 195/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1040695.5000 - accuracy: 0.2721 - val_loss: 1062289.3750 - val_accuracy: 0.0634\n",
      "Epoch 196/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 750586.8750 - accuracy: 0.2537 - val_loss: 830688.3125 - val_accuracy: 0.0634\n",
      "Epoch 197/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1063798.3750 - accuracy: 0.2745 - val_loss: 1140965.8750 - val_accuracy: 0.4293\n",
      "Epoch 198/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1163364.7500 - accuracy: 0.2953 - val_loss: 1930677.7500 - val_accuracy: 0.2098\n",
      "Epoch 199/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1672763.8750 - accuracy: 0.2451 - val_loss: 1330396.8750 - val_accuracy: 0.2098\n",
      "Epoch 200/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1149056.7500 - accuracy: 0.3015 - val_loss: 1377261.8750 - val_accuracy: 0.4293\n",
      "Epoch 201/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1057535.1250 - accuracy: 0.2770 - val_loss: 935142.0625 - val_accuracy: 0.4293\n",
      "Epoch 202/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1175858.8750 - accuracy: 0.2978 - val_loss: 1773264.1250 - val_accuracy: 0.4293\n",
      "Epoch 203/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1214881.7500 - accuracy: 0.2635 - val_loss: 1817556.0000 - val_accuracy: 0.1902\n",
      "Epoch 204/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 921342.0625 - accuracy: 0.2549 - val_loss: 768111.0625 - val_accuracy: 0.2098\n",
      "Epoch 205/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 472985.9062 - accuracy: 0.2978 - val_loss: 892095.8750 - val_accuracy: 0.1073\n",
      "Epoch 206/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 774476.8125 - accuracy: 0.2684 - val_loss: 1742323.3750 - val_accuracy: 0.1902\n",
      "Epoch 207/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 884998.7500 - accuracy: 0.2855 - val_loss: 1168478.7500 - val_accuracy: 0.4293\n",
      "Epoch 208/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 943547.1250 - accuracy: 0.2953 - val_loss: 1091195.1250 - val_accuracy: 0.1902\n",
      "Epoch 209/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 658965.8750 - accuracy: 0.2512 - val_loss: 790678.1875 - val_accuracy: 0.2098\n",
      "Epoch 210/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 682463.1875 - accuracy: 0.2757 - val_loss: 841961.9375 - val_accuracy: 0.4293\n",
      "Epoch 211/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 760791.6875 - accuracy: 0.2843 - val_loss: 1860270.6250 - val_accuracy: 0.1073\n",
      "Epoch 212/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 921981.6250 - accuracy: 0.2647 - val_loss: 582589.5000 - val_accuracy: 0.1073\n",
      "Epoch 213/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 870876.3750 - accuracy: 0.2708 - val_loss: 378988.4375 - val_accuracy: 0.1902\n",
      "Epoch 214/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 672419.1875 - accuracy: 0.2353 - val_loss: 879034.3750 - val_accuracy: 0.2098\n",
      "Epoch 215/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 668683.1250 - accuracy: 0.2672 - val_loss: 1177734.2500 - val_accuracy: 0.0634\n",
      "Epoch 216/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 877583.5000 - accuracy: 0.3015 - val_loss: 1066946.1250 - val_accuracy: 0.1073\n",
      "Epoch 217/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 677642.7500 - accuracy: 0.2561 - val_loss: 595056.0625 - val_accuracy: 0.2098\n",
      "Epoch 218/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 650929.3750 - accuracy: 0.3002 - val_loss: 776936.5625 - val_accuracy: 0.1073\n",
      "Epoch 219/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 818862.8750 - accuracy: 0.2966 - val_loss: 488537.4375 - val_accuracy: 0.1902\n",
      "Epoch 220/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 698885.0000 - accuracy: 0.2770 - val_loss: 970134.2500 - val_accuracy: 0.2098\n",
      "Epoch 221/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 722452.9375 - accuracy: 0.2782 - val_loss: 1029736.3750 - val_accuracy: 0.1902\n",
      "Epoch 222/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1298950.5000 - accuracy: 0.2353 - val_loss: 986397.1875 - val_accuracy: 0.4293\n",
      "Epoch 223/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 870684.9375 - accuracy: 0.2696 - val_loss: 571673.6250 - val_accuracy: 0.2098\n",
      "Epoch 224/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 968759.7500 - accuracy: 0.2843 - val_loss: 288576.1562 - val_accuracy: 0.2098\n",
      "Epoch 225/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 549032.0625 - accuracy: 0.3027 - val_loss: 1166510.8750 - val_accuracy: 0.1073\n",
      "Epoch 226/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 754083.5000 - accuracy: 0.2696 - val_loss: 723478.3750 - val_accuracy: 0.4293\n",
      "Epoch 227/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 932900.3125 - accuracy: 0.2782 - val_loss: 631589.0625 - val_accuracy: 0.2098\n",
      "Epoch 228/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 798972.1875 - accuracy: 0.2831 - val_loss: 1618844.1250 - val_accuracy: 0.4293\n",
      "Epoch 229/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 743624.9375 - accuracy: 0.2757 - val_loss: 1529084.0000 - val_accuracy: 0.1902\n",
      "Epoch 230/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 785334.5625 - accuracy: 0.3113 - val_loss: 689235.6875 - val_accuracy: 0.2098\n",
      "Epoch 231/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 582443.0000 - accuracy: 0.2892 - val_loss: 946548.6250 - val_accuracy: 0.2098\n",
      "Epoch 232/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1040401.4375 - accuracy: 0.2708 - val_loss: 986360.6875 - val_accuracy: 0.4293\n",
      "Epoch 233/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 729648.0625 - accuracy: 0.2757 - val_loss: 634226.0000 - val_accuracy: 0.4293\n",
      "Epoch 234/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 652564.1875 - accuracy: 0.2868 - val_loss: 803665.3125 - val_accuracy: 0.0634\n",
      "Epoch 235/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1282955.6250 - accuracy: 0.2635 - val_loss: 1165841.1250 - val_accuracy: 0.1902\n",
      "Epoch 236/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1483304.0000 - accuracy: 0.2990 - val_loss: 799218.1250 - val_accuracy: 0.4293\n",
      "Epoch 237/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1446408.3750 - accuracy: 0.2635 - val_loss: 1148192.5000 - val_accuracy: 0.1902\n",
      "Epoch 238/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 928456.5625 - accuracy: 0.2929 - val_loss: 435027.4375 - val_accuracy: 0.1073\n",
      "Epoch 239/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 512606.1875 - accuracy: 0.2953 - val_loss: 527191.5625 - val_accuracy: 0.1902\n",
      "Epoch 240/500\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 961632.6250 - accuracy: 0.2586 - val_loss: 529526.9375 - val_accuracy: 0.4293\n",
      "Epoch 241/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 883898.1250 - accuracy: 0.2598 - val_loss: 1033114.4375 - val_accuracy: 0.1073\n",
      "Epoch 242/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 724642.8750 - accuracy: 0.2917 - val_loss: 604513.1875 - val_accuracy: 0.2098\n",
      "Epoch 243/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 594561.3125 - accuracy: 0.2806 - val_loss: 1158933.8750 - val_accuracy: 0.1902\n",
      "Epoch 244/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 805327.3125 - accuracy: 0.2426 - val_loss: 649326.9375 - val_accuracy: 0.0634\n",
      "Epoch 245/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 519243.6562 - accuracy: 0.2623 - val_loss: 479592.7812 - val_accuracy: 0.4293\n",
      "Epoch 246/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 590720.5625 - accuracy: 0.2463 - val_loss: 760112.0625 - val_accuracy: 0.2098\n",
      "Epoch 247/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 770056.9375 - accuracy: 0.2488 - val_loss: 409174.0625 - val_accuracy: 0.4293\n",
      "Epoch 248/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 883817.6250 - accuracy: 0.2561 - val_loss: 694215.6250 - val_accuracy: 0.1073\n",
      "Epoch 249/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 757605.6250 - accuracy: 0.2843 - val_loss: 885738.0000 - val_accuracy: 0.4293\n",
      "Epoch 250/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 901923.3125 - accuracy: 0.2782 - val_loss: 1124100.0000 - val_accuracy: 0.2098\n",
      "Epoch 251/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 593544.3750 - accuracy: 0.2574 - val_loss: 1184515.8750 - val_accuracy: 0.1902\n",
      "Epoch 252/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 599333.8125 - accuracy: 0.2794 - val_loss: 595090.1250 - val_accuracy: 0.4293\n",
      "Epoch 253/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 592628.3125 - accuracy: 0.2868 - val_loss: 452094.2188 - val_accuracy: 0.1073\n",
      "Epoch 254/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 685107.7500 - accuracy: 0.2586 - val_loss: 393024.6875 - val_accuracy: 0.0634\n",
      "Epoch 255/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 682802.8750 - accuracy: 0.2904 - val_loss: 849173.3750 - val_accuracy: 0.1902\n",
      "Epoch 256/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 947100.8750 - accuracy: 0.2635 - val_loss: 1613133.2500 - val_accuracy: 0.1073\n",
      "Epoch 257/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1309479.0000 - accuracy: 0.2426 - val_loss: 897623.4375 - val_accuracy: 0.2098\n",
      "Epoch 258/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 973816.8125 - accuracy: 0.2868 - val_loss: 843749.4375 - val_accuracy: 0.1902\n",
      "Epoch 259/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 721624.6250 - accuracy: 0.2696 - val_loss: 638147.0000 - val_accuracy: 0.4293\n",
      "Epoch 260/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 562774.8125 - accuracy: 0.2917 - val_loss: 154588.4844 - val_accuracy: 0.1902\n",
      "Epoch 261/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 582842.1250 - accuracy: 0.2525 - val_loss: 495805.1562 - val_accuracy: 0.4293\n",
      "Epoch 262/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 496945.7500 - accuracy: 0.2831 - val_loss: 834306.8125 - val_accuracy: 0.4293\n",
      "Epoch 263/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 504495.5000 - accuracy: 0.2696 - val_loss: 803627.5625 - val_accuracy: 0.4293\n",
      "Epoch 264/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 585716.1875 - accuracy: 0.2868 - val_loss: 507005.7812 - val_accuracy: 0.1073\n",
      "Epoch 265/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 824934.0625 - accuracy: 0.2684 - val_loss: 608712.0625 - val_accuracy: 0.0634\n",
      "Epoch 266/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 660619.1250 - accuracy: 0.3199 - val_loss: 644749.3125 - val_accuracy: 0.2098\n",
      "Epoch 267/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 663144.9375 - accuracy: 0.2426 - val_loss: 633888.2500 - val_accuracy: 0.4293\n",
      "Epoch 268/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 680453.9375 - accuracy: 0.2978 - val_loss: 1397482.5000 - val_accuracy: 0.2098\n",
      "Epoch 269/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 898986.7500 - accuracy: 0.2390 - val_loss: 806421.9375 - val_accuracy: 0.2098\n",
      "Epoch 270/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 822704.1875 - accuracy: 0.3039 - val_loss: 672908.9375 - val_accuracy: 0.2098\n",
      "Epoch 271/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 686864.0000 - accuracy: 0.2696 - val_loss: 1064245.3750 - val_accuracy: 0.4293\n",
      "Epoch 272/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 880605.4375 - accuracy: 0.2708 - val_loss: 916465.3125 - val_accuracy: 0.1902\n",
      "Epoch 273/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 921176.5625 - accuracy: 0.2512 - val_loss: 1570193.1250 - val_accuracy: 0.4293\n",
      "Epoch 274/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 873702.1250 - accuracy: 0.3199 - val_loss: 679584.1250 - val_accuracy: 0.1902\n",
      "Epoch 275/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 805429.8750 - accuracy: 0.2475 - val_loss: 733156.5000 - val_accuracy: 0.1902\n",
      "Epoch 276/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 565087.3125 - accuracy: 0.2721 - val_loss: 169135.5781 - val_accuracy: 0.1902\n",
      "Epoch 277/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 519362.5000 - accuracy: 0.2684 - val_loss: 563639.5000 - val_accuracy: 0.4293\n",
      "Epoch 278/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 688595.3125 - accuracy: 0.3186 - val_loss: 928287.8750 - val_accuracy: 0.1073\n",
      "Epoch 279/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 498032.2500 - accuracy: 0.2708 - val_loss: 481413.5938 - val_accuracy: 0.4293\n",
      "Epoch 280/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 538444.8750 - accuracy: 0.2733 - val_loss: 525963.5625 - val_accuracy: 0.2098\n",
      "Epoch 281/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 504053.0625 - accuracy: 0.2549 - val_loss: 510478.0938 - val_accuracy: 0.1902\n",
      "Epoch 282/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 511835.6875 - accuracy: 0.2855 - val_loss: 509408.3438 - val_accuracy: 0.1073\n",
      "Epoch 283/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 982730.3750 - accuracy: 0.2353 - val_loss: 1760974.0000 - val_accuracy: 0.2098\n",
      "Epoch 284/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1119391.1250 - accuracy: 0.2941 - val_loss: 610228.1250 - val_accuracy: 0.4293\n",
      "Epoch 285/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 641126.7500 - accuracy: 0.2659 - val_loss: 831282.4375 - val_accuracy: 0.1902\n",
      "Epoch 286/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 688834.5000 - accuracy: 0.2561 - val_loss: 1357269.2500 - val_accuracy: 0.1902\n",
      "Epoch 287/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 541919.0000 - accuracy: 0.2917 - val_loss: 578704.5625 - val_accuracy: 0.4293\n",
      "Epoch 288/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 417916.6250 - accuracy: 0.3100 - val_loss: 427780.6562 - val_accuracy: 0.1902\n",
      "Epoch 289/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 836781.3125 - accuracy: 0.2831 - val_loss: 985747.0625 - val_accuracy: 0.4293\n",
      "Epoch 290/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 648524.5625 - accuracy: 0.2806 - val_loss: 421284.8125 - val_accuracy: 0.1902\n",
      "Epoch 291/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 570775.0625 - accuracy: 0.2623 - val_loss: 764465.3750 - val_accuracy: 0.1902\n",
      "Epoch 292/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 693312.8125 - accuracy: 0.2696 - val_loss: 583661.5625 - val_accuracy: 0.4293\n",
      "Epoch 293/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 864500.6250 - accuracy: 0.2684 - val_loss: 1166107.5000 - val_accuracy: 0.2098\n",
      "Epoch 294/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 975600.9375 - accuracy: 0.2770 - val_loss: 1260450.8750 - val_accuracy: 0.2098\n",
      "Epoch 295/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1049914.2500 - accuracy: 0.2721 - val_loss: 861885.3750 - val_accuracy: 0.2098\n",
      "Epoch 296/500\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 703853.5000 - accuracy: 0.2659 - val_loss: 848264.7500 - val_accuracy: 0.4293\n",
      "Epoch 297/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 772573.6250 - accuracy: 0.2782 - val_loss: 787542.0000 - val_accuracy: 0.4293\n",
      "Epoch 298/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 623478.8125 - accuracy: 0.2966 - val_loss: 267852.2812 - val_accuracy: 0.0634\n",
      "Epoch 299/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 445444.5625 - accuracy: 0.2525 - val_loss: 1019466.6250 - val_accuracy: 0.1902\n",
      "Epoch 300/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 679825.8750 - accuracy: 0.2892 - val_loss: 425690.3750 - val_accuracy: 0.4293\n",
      "Epoch 301/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 578184.0625 - accuracy: 0.2904 - val_loss: 443725.6562 - val_accuracy: 0.1902\n",
      "Epoch 302/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 403757.9062 - accuracy: 0.2831 - val_loss: 613008.0625 - val_accuracy: 0.4293\n",
      "Epoch 303/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 588130.1875 - accuracy: 0.2855 - val_loss: 482363.5000 - val_accuracy: 0.4293\n",
      "Epoch 304/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 538623.7500 - accuracy: 0.2623 - val_loss: 813082.6875 - val_accuracy: 0.4293\n",
      "Epoch 305/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 720082.1250 - accuracy: 0.2733 - val_loss: 1040369.8125 - val_accuracy: 0.2098\n",
      "Epoch 306/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 884747.6875 - accuracy: 0.2843 - val_loss: 505632.7500 - val_accuracy: 0.4293\n",
      "Epoch 307/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 540687.6250 - accuracy: 0.2733 - val_loss: 765706.7500 - val_accuracy: 0.2098\n",
      "Epoch 308/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 615543.0000 - accuracy: 0.3064 - val_loss: 466381.3750 - val_accuracy: 0.1902\n",
      "Epoch 309/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 803240.2500 - accuracy: 0.2574 - val_loss: 595486.0625 - val_accuracy: 0.1073\n",
      "Epoch 310/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 521186.2500 - accuracy: 0.2574 - val_loss: 320269.1250 - val_accuracy: 0.4293\n",
      "Epoch 311/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 909102.1250 - accuracy: 0.2623 - val_loss: 502353.9375 - val_accuracy: 0.4293\n",
      "Epoch 312/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 647603.6250 - accuracy: 0.2733 - val_loss: 334760.1875 - val_accuracy: 0.4293\n",
      "Epoch 313/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 534105.5000 - accuracy: 0.2990 - val_loss: 756691.2500 - val_accuracy: 0.1902\n",
      "Epoch 314/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 481082.2812 - accuracy: 0.2941 - val_loss: 502073.3750 - val_accuracy: 0.1902\n",
      "Epoch 315/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 409755.3438 - accuracy: 0.2623 - val_loss: 525620.1250 - val_accuracy: 0.1902\n",
      "Epoch 316/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 779247.8125 - accuracy: 0.2843 - val_loss: 688864.4375 - val_accuracy: 0.1073\n",
      "Epoch 317/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 670601.7500 - accuracy: 0.2966 - val_loss: 339633.7188 - val_accuracy: 0.4293\n",
      "Epoch 318/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 552945.0625 - accuracy: 0.3137 - val_loss: 69197.9141 - val_accuracy: 0.1902\n",
      "Epoch 319/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 281652.7500 - accuracy: 0.2292 - val_loss: 176665.6719 - val_accuracy: 0.4293\n",
      "Epoch 320/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 570244.3750 - accuracy: 0.2475 - val_loss: 905210.3125 - val_accuracy: 0.1902\n",
      "Epoch 321/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 723850.7500 - accuracy: 0.2488 - val_loss: 703408.6250 - val_accuracy: 0.4293\n",
      "Epoch 322/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 637911.1875 - accuracy: 0.2525 - val_loss: 282525.2188 - val_accuracy: 0.4293\n",
      "Epoch 323/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 285641.7500 - accuracy: 0.2880 - val_loss: 443696.1562 - val_accuracy: 0.2098\n",
      "Epoch 324/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 498655.8438 - accuracy: 0.2586 - val_loss: 439209.8750 - val_accuracy: 0.1902\n",
      "Epoch 325/500\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 488882.9688 - accuracy: 0.2672 - val_loss: 768544.3750 - val_accuracy: 0.1902\n",
      "Epoch 326/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 474187.4375 - accuracy: 0.2904 - val_loss: 217420.2344 - val_accuracy: 0.4293\n",
      "Epoch 327/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 626937.8125 - accuracy: 0.2390 - val_loss: 399373.1875 - val_accuracy: 0.4293\n",
      "Epoch 328/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 488767.2812 - accuracy: 0.2855 - val_loss: 305788.4688 - val_accuracy: 0.1902\n",
      "Epoch 329/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 507648.4375 - accuracy: 0.2733 - val_loss: 635401.6875 - val_accuracy: 0.1073\n",
      "Epoch 330/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 641642.9375 - accuracy: 0.2843 - val_loss: 504707.2812 - val_accuracy: 0.4293\n",
      "Epoch 331/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 577622.5000 - accuracy: 0.2684 - val_loss: 820084.3125 - val_accuracy: 0.0634\n",
      "Epoch 332/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 450062.5000 - accuracy: 0.2365 - val_loss: 333649.1562 - val_accuracy: 0.4293\n",
      "Epoch 333/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 406711.9688 - accuracy: 0.2733 - val_loss: 601441.5000 - val_accuracy: 0.4293\n",
      "Epoch 334/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 511590.0938 - accuracy: 0.2659 - val_loss: 619109.7500 - val_accuracy: 0.4293\n",
      "Epoch 335/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 756321.9375 - accuracy: 0.2843 - val_loss: 629081.0000 - val_accuracy: 0.1902\n",
      "Epoch 336/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 727768.0625 - accuracy: 0.2598 - val_loss: 564221.2500 - val_accuracy: 0.1902\n",
      "Epoch 337/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 617346.1250 - accuracy: 0.2757 - val_loss: 646665.6250 - val_accuracy: 0.4293\n",
      "Epoch 338/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 796198.0625 - accuracy: 0.2941 - val_loss: 910953.7500 - val_accuracy: 0.2098\n",
      "Epoch 339/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 687878.6875 - accuracy: 0.2574 - val_loss: 473256.5000 - val_accuracy: 0.1902\n",
      "Epoch 340/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 404313.8125 - accuracy: 0.3002 - val_loss: 741683.1875 - val_accuracy: 0.2098\n",
      "Epoch 341/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 375287.5312 - accuracy: 0.2855 - val_loss: 324771.5312 - val_accuracy: 0.2098\n",
      "Epoch 342/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 418385.4062 - accuracy: 0.2880 - val_loss: 301734.1562 - val_accuracy: 0.0634\n",
      "Epoch 343/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 659763.8125 - accuracy: 0.2904 - val_loss: 569735.6250 - val_accuracy: 0.1902\n",
      "Epoch 344/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 622072.6875 - accuracy: 0.2831 - val_loss: 1350572.5000 - val_accuracy: 0.1902\n",
      "Epoch 345/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 645241.1250 - accuracy: 0.2941 - val_loss: 143772.6875 - val_accuracy: 0.1073\n",
      "Epoch 346/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 412543.1250 - accuracy: 0.2672 - val_loss: 546352.3750 - val_accuracy: 0.1902\n",
      "Epoch 347/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 503870.7500 - accuracy: 0.2757 - val_loss: 508827.0000 - val_accuracy: 0.2098\n",
      "Epoch 348/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 523517.8125 - accuracy: 0.2806 - val_loss: 466773.6250 - val_accuracy: 0.4293\n",
      "Epoch 349/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 440727.0938 - accuracy: 0.2782 - val_loss: 529236.0000 - val_accuracy: 0.1073\n",
      "Epoch 350/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 326623.8125 - accuracy: 0.2635 - val_loss: 219267.7500 - val_accuracy: 0.1073\n",
      "Epoch 351/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 407410.0000 - accuracy: 0.2696 - val_loss: 651136.4375 - val_accuracy: 0.1073\n",
      "Epoch 352/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 481296.7500 - accuracy: 0.2966 - val_loss: 413664.9688 - val_accuracy: 0.1073\n",
      "Epoch 353/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 517454.1875 - accuracy: 0.2684 - val_loss: 609839.2500 - val_accuracy: 0.4293\n",
      "Epoch 354/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 513094.1875 - accuracy: 0.2684 - val_loss: 888175.4375 - val_accuracy: 0.2098\n",
      "Epoch 355/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 611013.6250 - accuracy: 0.2708 - val_loss: 750509.8750 - val_accuracy: 0.2098\n",
      "Epoch 356/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 827224.1875 - accuracy: 0.2880 - val_loss: 1430473.6250 - val_accuracy: 0.4293\n",
      "Epoch 357/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 831607.9375 - accuracy: 0.2831 - val_loss: 494327.9688 - val_accuracy: 0.0634\n",
      "Epoch 358/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 444072.7188 - accuracy: 0.2721 - val_loss: 532879.9375 - val_accuracy: 0.2098\n",
      "Epoch 359/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 623691.8125 - accuracy: 0.2390 - val_loss: 760938.3750 - val_accuracy: 0.1902\n",
      "Epoch 360/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 449067.8438 - accuracy: 0.2757 - val_loss: 244548.5625 - val_accuracy: 0.2098\n",
      "Epoch 361/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 465084.5625 - accuracy: 0.3088 - val_loss: 239915.2656 - val_accuracy: 0.1902\n",
      "Epoch 362/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 467804.8125 - accuracy: 0.2402 - val_loss: 556938.6875 - val_accuracy: 0.2098\n",
      "Epoch 363/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 325337.7500 - accuracy: 0.2806 - val_loss: 156423.4688 - val_accuracy: 0.4293\n",
      "Epoch 364/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 387603.7188 - accuracy: 0.2806 - val_loss: 323357.4688 - val_accuracy: 0.1902\n",
      "Epoch 365/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 430323.3750 - accuracy: 0.2488 - val_loss: 157523.9062 - val_accuracy: 0.0634\n",
      "Epoch 366/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 531554.1875 - accuracy: 0.2500 - val_loss: 909242.0625 - val_accuracy: 0.1902\n",
      "Epoch 367/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 468283.8438 - accuracy: 0.2598 - val_loss: 244497.3125 - val_accuracy: 0.1902\n",
      "Epoch 368/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 725001.6250 - accuracy: 0.2243 - val_loss: 738858.0625 - val_accuracy: 0.4293\n",
      "Epoch 369/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 993755.7500 - accuracy: 0.2929 - val_loss: 760881.6250 - val_accuracy: 0.1073\n",
      "Epoch 370/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 378987.2188 - accuracy: 0.2819 - val_loss: 502920.8125 - val_accuracy: 0.1902\n",
      "Epoch 371/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 509058.1875 - accuracy: 0.2439 - val_loss: 250368.6406 - val_accuracy: 0.4293\n",
      "Epoch 372/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 308689.5625 - accuracy: 0.3015 - val_loss: 244044.4844 - val_accuracy: 0.4293\n",
      "Epoch 373/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 527621.3750 - accuracy: 0.2623 - val_loss: 692295.7500 - val_accuracy: 0.4293\n",
      "Epoch 374/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 560220.8750 - accuracy: 0.2868 - val_loss: 313167.0000 - val_accuracy: 0.4293\n",
      "Epoch 375/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 444671.2812 - accuracy: 0.3088 - val_loss: 174176.5312 - val_accuracy: 0.2098\n",
      "Epoch 376/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 288198.3438 - accuracy: 0.2745 - val_loss: 179671.1562 - val_accuracy: 0.4293\n",
      "Epoch 377/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 513346.6562 - accuracy: 0.2745 - val_loss: 967308.7500 - val_accuracy: 0.2098\n",
      "Epoch 378/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 433072.1875 - accuracy: 0.2978 - val_loss: 469240.7812 - val_accuracy: 0.4293\n",
      "Epoch 379/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 501342.6562 - accuracy: 0.2659 - val_loss: 310657.8750 - val_accuracy: 0.4293\n",
      "Epoch 380/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 352335.3750 - accuracy: 0.2892 - val_loss: 303113.6875 - val_accuracy: 0.4293\n",
      "Epoch 381/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 376257.2812 - accuracy: 0.2586 - val_loss: 193065.1875 - val_accuracy: 0.2098\n",
      "Epoch 382/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 618148.5625 - accuracy: 0.2475 - val_loss: 604191.3750 - val_accuracy: 0.4293\n",
      "Epoch 383/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 657878.8125 - accuracy: 0.3260 - val_loss: 361050.0000 - val_accuracy: 0.1902\n",
      "Epoch 384/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 433964.9688 - accuracy: 0.2794 - val_loss: 633050.7500 - val_accuracy: 0.2098\n",
      "Epoch 385/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 506337.0938 - accuracy: 0.2451 - val_loss: 367523.5000 - val_accuracy: 0.4293\n",
      "Epoch 386/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 492561.5938 - accuracy: 0.3113 - val_loss: 722727.1875 - val_accuracy: 0.2098\n",
      "Epoch 387/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 542753.5000 - accuracy: 0.2757 - val_loss: 513068.5312 - val_accuracy: 0.0634\n",
      "Epoch 388/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 236771.7969 - accuracy: 0.2659 - val_loss: 234712.0781 - val_accuracy: 0.1902\n",
      "Epoch 389/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 282110.2812 - accuracy: 0.2843 - val_loss: 590062.6250 - val_accuracy: 0.2098\n",
      "Epoch 390/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 619717.8750 - accuracy: 0.2733 - val_loss: 359692.0625 - val_accuracy: 0.2098\n",
      "Epoch 391/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 479522.7188 - accuracy: 0.2782 - val_loss: 505618.0312 - val_accuracy: 0.4293\n",
      "Epoch 392/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 374697.4375 - accuracy: 0.2561 - val_loss: 242547.6875 - val_accuracy: 0.0634\n",
      "Epoch 393/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 744144.5625 - accuracy: 0.2696 - val_loss: 1422380.6250 - val_accuracy: 0.1902\n",
      "Epoch 394/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 815134.5000 - accuracy: 0.2439 - val_loss: 817226.2500 - val_accuracy: 0.1902\n",
      "Epoch 395/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 767459.3750 - accuracy: 0.2843 - val_loss: 305269.0625 - val_accuracy: 0.1902\n",
      "Epoch 396/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 466343.0938 - accuracy: 0.2659 - val_loss: 572595.0625 - val_accuracy: 0.2098\n",
      "Epoch 397/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 225202.0938 - accuracy: 0.2561 - val_loss: 96907.3828 - val_accuracy: 0.4293\n",
      "Epoch 398/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 331976.5938 - accuracy: 0.2733 - val_loss: 249840.2344 - val_accuracy: 0.2098\n",
      "Epoch 399/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 236594.7031 - accuracy: 0.2475 - val_loss: 337800.3750 - val_accuracy: 0.1902\n",
      "Epoch 400/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 327981.8750 - accuracy: 0.2953 - val_loss: 166196.8750 - val_accuracy: 0.4293\n",
      "Epoch 401/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 231779.3750 - accuracy: 0.2390 - val_loss: 322120.6875 - val_accuracy: 0.4293\n",
      "Epoch 402/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 430669.6562 - accuracy: 0.2990 - val_loss: 375915.4688 - val_accuracy: 0.0634\n",
      "Epoch 403/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 585434.3750 - accuracy: 0.2880 - val_loss: 224583.1562 - val_accuracy: 0.0634\n",
      "Epoch 404/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 452660.5000 - accuracy: 0.2512 - val_loss: 566288.8750 - val_accuracy: 0.1902\n",
      "Epoch 405/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 505180.0000 - accuracy: 0.2684 - val_loss: 432964.6875 - val_accuracy: 0.1902\n",
      "Epoch 406/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 406719.4375 - accuracy: 0.2770 - val_loss: 192939.2812 - val_accuracy: 0.1073\n",
      "Epoch 407/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 289966.5625 - accuracy: 0.2659 - val_loss: 502839.7188 - val_accuracy: 0.4293\n",
      "Epoch 408/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 453693.7500 - accuracy: 0.2892 - val_loss: 100024.7500 - val_accuracy: 0.1902\n",
      "Epoch 409/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 253081.7656 - accuracy: 0.2806 - val_loss: 297283.0938 - val_accuracy: 0.1902\n",
      "Epoch 410/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 339312.3438 - accuracy: 0.2623 - val_loss: 136135.1094 - val_accuracy: 0.4293\n",
      "Epoch 411/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 310571.0625 - accuracy: 0.2598 - val_loss: 324917.7500 - val_accuracy: 0.4293\n",
      "Epoch 412/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 530499.7500 - accuracy: 0.2855 - val_loss: 316593.2500 - val_accuracy: 0.2098\n",
      "Epoch 413/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 238460.1406 - accuracy: 0.2721 - val_loss: 96935.6797 - val_accuracy: 0.1902\n",
      "Epoch 414/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 203384.8750 - accuracy: 0.2733 - val_loss: 264758.2188 - val_accuracy: 0.2098\n",
      "Epoch 415/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 346921.4062 - accuracy: 0.2525 - val_loss: 337900.6562 - val_accuracy: 0.4293\n",
      "Epoch 416/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 324868.8125 - accuracy: 0.2855 - val_loss: 186175.1094 - val_accuracy: 0.1902\n",
      "Epoch 417/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 252610.2812 - accuracy: 0.3051 - val_loss: 516380.1875 - val_accuracy: 0.4293\n",
      "Epoch 418/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 572349.0625 - accuracy: 0.2525 - val_loss: 178078.5938 - val_accuracy: 0.4293\n",
      "Epoch 419/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 615295.8750 - accuracy: 0.2782 - val_loss: 670608.9375 - val_accuracy: 0.4293\n",
      "Epoch 420/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 357423.0312 - accuracy: 0.2549 - val_loss: 288598.7812 - val_accuracy: 0.1073\n",
      "Epoch 421/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 350787.4062 - accuracy: 0.2794 - val_loss: 554955.4375 - val_accuracy: 0.2098\n",
      "Epoch 422/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 455025.7500 - accuracy: 0.2929 - val_loss: 661375.3125 - val_accuracy: 0.1902\n",
      "Epoch 423/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 220216.9062 - accuracy: 0.2745 - val_loss: 306019.6562 - val_accuracy: 0.1073\n",
      "Epoch 424/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 333755.5625 - accuracy: 0.2647 - val_loss: 491578.6562 - val_accuracy: 0.2098\n",
      "Epoch 425/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 284034.0938 - accuracy: 0.2365 - val_loss: 281856.7500 - val_accuracy: 0.1902\n",
      "Epoch 426/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 327491.0625 - accuracy: 0.2708 - val_loss: 366169.8750 - val_accuracy: 0.4293\n",
      "Epoch 427/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 170573.7969 - accuracy: 0.2610 - val_loss: 209639.1875 - val_accuracy: 0.1073\n",
      "Epoch 428/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 161703.3125 - accuracy: 0.2782 - val_loss: 86294.1250 - val_accuracy: 0.1902\n",
      "Epoch 429/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 263976.8750 - accuracy: 0.2708 - val_loss: 187934.4062 - val_accuracy: 0.4293\n",
      "Epoch 430/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 289510.4062 - accuracy: 0.2941 - val_loss: 966231.6250 - val_accuracy: 0.2098\n",
      "Epoch 431/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 652711.9375 - accuracy: 0.2635 - val_loss: 244186.5000 - val_accuracy: 0.4293\n",
      "Epoch 432/500\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 295297.0312 - accuracy: 0.2549 - val_loss: 273306.4062 - val_accuracy: 0.4293\n",
      "Epoch 433/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 249241.6250 - accuracy: 0.2819 - val_loss: 726352.6250 - val_accuracy: 0.2098\n",
      "Epoch 434/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 331747.4375 - accuracy: 0.2684 - val_loss: 193944.5781 - val_accuracy: 0.2098\n",
      "Epoch 435/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 365284.6250 - accuracy: 0.1973 - val_loss: 508340.8438 - val_accuracy: 0.1902\n",
      "Epoch 436/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 605339.8750 - accuracy: 0.3174 - val_loss: 656505.0000 - val_accuracy: 0.1902\n",
      "Epoch 437/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 233687.3906 - accuracy: 0.2757 - val_loss: 156285.4375 - val_accuracy: 0.4293\n",
      "Epoch 438/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 183533.1406 - accuracy: 0.3162 - val_loss: 181808.1562 - val_accuracy: 0.4293\n",
      "Epoch 439/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 252993.8594 - accuracy: 0.2757 - val_loss: 57287.0820 - val_accuracy: 0.4293\n",
      "Epoch 440/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 293445.5625 - accuracy: 0.2782 - val_loss: 107494.2734 - val_accuracy: 0.1902\n",
      "Epoch 441/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 212866.2188 - accuracy: 0.2561 - val_loss: 144172.0625 - val_accuracy: 0.4293\n",
      "Epoch 442/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 215170.7031 - accuracy: 0.2819 - val_loss: 131629.6250 - val_accuracy: 0.4293\n",
      "Epoch 443/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 221375.0625 - accuracy: 0.2745 - val_loss: 263014.9688 - val_accuracy: 0.1073\n",
      "Epoch 444/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 171496.2969 - accuracy: 0.2672 - val_loss: 109199.2266 - val_accuracy: 0.4293\n",
      "Epoch 445/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 262797.0938 - accuracy: 0.2721 - val_loss: 615238.0000 - val_accuracy: 0.2098\n",
      "Epoch 446/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 327347.9375 - accuracy: 0.2561 - val_loss: 198292.1719 - val_accuracy: 0.1902\n",
      "Epoch 447/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 330616.9062 - accuracy: 0.2561 - val_loss: 200738.0156 - val_accuracy: 0.2098\n",
      "Epoch 448/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 286410.8438 - accuracy: 0.3100 - val_loss: 418705.9375 - val_accuracy: 0.1902\n",
      "Epoch 449/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 274688.1250 - accuracy: 0.2708 - val_loss: 278780.7188 - val_accuracy: 0.4293\n",
      "Epoch 450/500\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 258928.0000 - accuracy: 0.2819 - val_loss: 631334.0000 - val_accuracy: 0.2098\n",
      "Epoch 451/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 485702.8125 - accuracy: 0.2684 - val_loss: 368017.1562 - val_accuracy: 0.0634\n",
      "Epoch 452/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 391620.4375 - accuracy: 0.2953 - val_loss: 434628.8750 - val_accuracy: 0.2098\n",
      "Epoch 453/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 150977.8906 - accuracy: 0.2684 - val_loss: 206437.1719 - val_accuracy: 0.1073\n",
      "Epoch 454/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 206717.7812 - accuracy: 0.2721 - val_loss: 163178.4375 - val_accuracy: 0.0634\n",
      "Epoch 455/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 158334.3750 - accuracy: 0.2904 - val_loss: 150369.2969 - val_accuracy: 0.1902\n",
      "Epoch 456/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 463432.9062 - accuracy: 0.2574 - val_loss: 242576.7344 - val_accuracy: 0.4293\n",
      "Epoch 457/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 403605.5938 - accuracy: 0.2696 - val_loss: 484169.1562 - val_accuracy: 0.4293\n",
      "Epoch 458/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 369120.3438 - accuracy: 0.2904 - val_loss: 847300.0625 - val_accuracy: 0.2098\n",
      "Epoch 459/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 446719.4062 - accuracy: 0.2990 - val_loss: 329186.6562 - val_accuracy: 0.1902\n",
      "Epoch 460/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 362340.9688 - accuracy: 0.2684 - val_loss: 280649.0000 - val_accuracy: 0.1902\n",
      "Epoch 461/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 258085.3750 - accuracy: 0.2561 - val_loss: 156938.0625 - val_accuracy: 0.1902\n",
      "Epoch 462/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 376924.5938 - accuracy: 0.2623 - val_loss: 473921.0938 - val_accuracy: 0.4293\n",
      "Epoch 463/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 231773.8594 - accuracy: 0.3039 - val_loss: 290784.1250 - val_accuracy: 0.4293\n",
      "Epoch 464/500\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 345754.0938 - accuracy: 0.2500 - val_loss: 86551.6016 - val_accuracy: 0.1902\n",
      "Epoch 465/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 334965.9688 - accuracy: 0.2892 - val_loss: 219572.7656 - val_accuracy: 0.4293\n",
      "Epoch 466/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 265575.3750 - accuracy: 0.2684 - val_loss: 529251.1875 - val_accuracy: 0.4293\n",
      "Epoch 467/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 340629.0938 - accuracy: 0.2978 - val_loss: 248001.1875 - val_accuracy: 0.1902\n",
      "Epoch 468/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 244016.2969 - accuracy: 0.2574 - val_loss: 233987.5156 - val_accuracy: 0.1902\n",
      "Epoch 469/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 147056.2188 - accuracy: 0.2708 - val_loss: 606065.2500 - val_accuracy: 0.2098\n",
      "Epoch 470/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 212548.9844 - accuracy: 0.3064 - val_loss: 69950.8125 - val_accuracy: 0.4293\n",
      "Epoch 471/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 265669.1250 - accuracy: 0.2745 - val_loss: 558473.7500 - val_accuracy: 0.2098\n",
      "Epoch 472/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 285514.5312 - accuracy: 0.2721 - val_loss: 169979.6875 - val_accuracy: 0.4293\n",
      "Epoch 473/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 203564.2031 - accuracy: 0.2745 - val_loss: 147208.8906 - val_accuracy: 0.4293\n",
      "Epoch 474/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 324530.1875 - accuracy: 0.2917 - val_loss: 317566.4062 - val_accuracy: 0.0634\n",
      "Epoch 475/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 256983.2188 - accuracy: 0.2365 - val_loss: 459414.2500 - val_accuracy: 0.4293\n",
      "Epoch 476/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 300415.3125 - accuracy: 0.2586 - val_loss: 166477.7031 - val_accuracy: 0.4293\n",
      "Epoch 477/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 234524.3750 - accuracy: 0.2770 - val_loss: 231013.7031 - val_accuracy: 0.2098\n",
      "Epoch 478/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 200783.9844 - accuracy: 0.2439 - val_loss: 272887.1250 - val_accuracy: 0.1073\n",
      "Epoch 479/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 180572.0938 - accuracy: 0.2635 - val_loss: 421423.7812 - val_accuracy: 0.2098\n",
      "Epoch 480/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 317303.5938 - accuracy: 0.2966 - val_loss: 616479.6875 - val_accuracy: 0.1902\n",
      "Epoch 481/500\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 314592.4688 - accuracy: 0.2598 - val_loss: 269948.2188 - val_accuracy: 0.4293\n",
      "Epoch 482/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 233189.1719 - accuracy: 0.2549 - val_loss: 210049.2500 - val_accuracy: 0.4293\n",
      "Epoch 483/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 252400.9375 - accuracy: 0.2806 - val_loss: 175257.5156 - val_accuracy: 0.1073\n",
      "Epoch 484/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 325408.6250 - accuracy: 0.2635 - val_loss: 262977.1250 - val_accuracy: 0.4293\n",
      "Epoch 485/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 223425.0625 - accuracy: 0.3088 - val_loss: 276150.8438 - val_accuracy: 0.1073\n",
      "Epoch 486/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 214112.7188 - accuracy: 0.2868 - val_loss: 93635.7500 - val_accuracy: 0.1902\n",
      "Epoch 487/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 238677.8594 - accuracy: 0.2623 - val_loss: 312994.1562 - val_accuracy: 0.2098\n",
      "Epoch 488/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 325855.6875 - accuracy: 0.2586 - val_loss: 344274.7812 - val_accuracy: 0.1902\n",
      "Epoch 489/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 204638.8594 - accuracy: 0.2978 - val_loss: 208008.9688 - val_accuracy: 0.4293\n",
      "Epoch 490/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 214495.5469 - accuracy: 0.2647 - val_loss: 301575.9375 - val_accuracy: 0.2098\n",
      "Epoch 491/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 255385.9219 - accuracy: 0.2659 - val_loss: 117219.4141 - val_accuracy: 0.4293\n",
      "Epoch 492/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 161299.0156 - accuracy: 0.2917 - val_loss: 219714.5938 - val_accuracy: 0.4293\n",
      "Epoch 493/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 237285.8438 - accuracy: 0.2426 - val_loss: 367229.4375 - val_accuracy: 0.1902\n",
      "Epoch 494/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 411590.6562 - accuracy: 0.2684 - val_loss: 945603.2500 - val_accuracy: 0.1073\n",
      "Epoch 495/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 490875.6562 - accuracy: 0.2610 - val_loss: 575272.0000 - val_accuracy: 0.4293\n",
      "Epoch 496/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 299748.7188 - accuracy: 0.2806 - val_loss: 574931.1250 - val_accuracy: 0.2098\n",
      "Epoch 497/500\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 309008.1562 - accuracy: 0.2316 - val_loss: 139037.9688 - val_accuracy: 0.4293\n",
      "Epoch 498/500\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 128315.6562 - accuracy: 0.2757 - val_loss: 49549.3398 - val_accuracy: 0.4293\n",
      "Epoch 499/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 116158.1250 - accuracy: 0.2623 - val_loss: 82550.4688 - val_accuracy: 0.4293\n",
      "Epoch 500/500\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 132112.6406 - accuracy: 0.2770 - val_loss: 206820.7656 - val_accuracy: 0.4293\n"
     ]
    }
   ],
   "source": [
    "# now we just update our model fit call\n",
    "history = model.fit(X,\n",
    "                    dummy_y,\n",
    "                    epochs=500, \n",
    "                    \n",
    "                   \n",
    "                    #batch_size=10,\n",
    "                    #shuffle=True,\n",
    "                    validation_split=0.2)\n",
    "                    #verbose=1\n",
    "                    #callbacks=[es],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_dict = history.history\n",
    "\n",
    "# learning curve\n",
    "# accuracy\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13848039507865906,\n",
       " 0.16299019753932953,\n",
       " 0.16911764442920685,\n",
       " 0.15808823704719543,\n",
       " 0.28799018263816833,\n",
       " 0.27328431606292725,\n",
       " 0.2340686321258545,\n",
       " 0.2647058963775635,\n",
       " 0.28921568393707275,\n",
       " 0.2757352888584137,\n",
       " 0.28921568393707275,\n",
       " 0.2916666567325592,\n",
       " 0.2904411852359772,\n",
       " 0.2720588147640228,\n",
       " 0.25735294818878174,\n",
       " 0.30514705181121826,\n",
       " 0.25735294818878174,\n",
       " 0.28308823704719543,\n",
       " 0.30024510622024536,\n",
       " 0.2855392098426819,\n",
       " 0.2720588147640228,\n",
       " 0.2696078419685364,\n",
       " 0.27450981736183167,\n",
       " 0.3026960790157318,\n",
       " 0.2708333432674408,\n",
       " 0.24877451360225677,\n",
       " 0.27328431606292725,\n",
       " 0.31495097279548645,\n",
       " 0.2769607901573181,\n",
       " 0.28431373834609985,\n",
       " 0.27818626165390015,\n",
       " 0.24387255311012268,\n",
       " 0.3075980246067047,\n",
       " 0.2696078419685364,\n",
       " 0.2708333432674408,\n",
       " 0.2855392098426819,\n",
       " 0.27450981736183167,\n",
       " 0.2696078419685364,\n",
       " 0.2401960790157318,\n",
       " 0.2659313678741455,\n",
       " 0.2647058963775635,\n",
       " 0.2916666567325592,\n",
       " 0.2855392098426819,\n",
       " 0.26838234066963196,\n",
       " 0.2757352888584137,\n",
       " 0.24264705181121826,\n",
       " 0.2671568691730499,\n",
       " 0.30024510622024536,\n",
       " 0.25367647409439087,\n",
       " 0.280637264251709,\n",
       " 0.2953431308269501,\n",
       " 0.24142156541347504,\n",
       " 0.25735294818878174,\n",
       " 0.26348039507865906,\n",
       " 0.3235294222831726,\n",
       " 0.23774509131908417,\n",
       " 0.28308823704719543,\n",
       " 0.281862735748291,\n",
       " 0.25857841968536377,\n",
       " 0.2757352888584137,\n",
       " 0.27818626165390015,\n",
       " 0.2769607901573181,\n",
       " 0.2671568691730499,\n",
       " 0.25367647409439087,\n",
       " 0.27941176295280457,\n",
       " 0.27450981736183167,\n",
       " 0.2708333432674408,\n",
       " 0.27818626165390015,\n",
       " 0.2659313678741455,\n",
       " 0.26838234066963196,\n",
       " 0.2450980395078659,\n",
       " 0.26225489377975464,\n",
       " 0.2904411852359772,\n",
       " 0.2647058963775635,\n",
       " 0.28799018263816833,\n",
       " 0.27941176295280457,\n",
       " 0.3174019753932953,\n",
       " 0.2549019753932953,\n",
       " 0.29411765933036804,\n",
       " 0.281862735748291,\n",
       " 0.25367647409439087,\n",
       " 0.280637264251709,\n",
       " 0.2696078419685364,\n",
       " 0.2904411852359772,\n",
       " 0.25245097279548645,\n",
       " 0.2867647111415863,\n",
       " 0.2855392098426819,\n",
       " 0.2671568691730499,\n",
       " 0.2769607901573181,\n",
       " 0.2512255012989044,\n",
       " 0.2977941036224365,\n",
       " 0.28921568393707275,\n",
       " 0.25,\n",
       " 0.2904411852359772,\n",
       " 0.2671568691730499,\n",
       " 0.2965686321258545,\n",
       " 0.2671568691730499,\n",
       " 0.2769607901573181,\n",
       " 0.2769607901573181,\n",
       " 0.25735294818878174,\n",
       " 0.2769607901573181,\n",
       " 0.2549019753932953,\n",
       " 0.280637264251709,\n",
       " 0.281862735748291,\n",
       " 0.2720588147640228,\n",
       " 0.2708333432674408,\n",
       " 0.2720588147640228,\n",
       " 0.2977941036224365,\n",
       " 0.28799018263816833,\n",
       " 0.27450981736183167,\n",
       " 0.2769607901573181,\n",
       " 0.25857841968536377,\n",
       " 0.3125,\n",
       " 0.2610294222831726,\n",
       " 0.28431373834609985,\n",
       " 0.27818626165390015,\n",
       " 0.2855392098426819,\n",
       " 0.2450980395078659,\n",
       " 0.2708333432674408,\n",
       " 0.28431373834609985,\n",
       " 0.23651960492134094,\n",
       " 0.29411765933036804,\n",
       " 0.2769607901573181,\n",
       " 0.27328431606292725,\n",
       " 0.24632352590560913,\n",
       " 0.30024510622024536,\n",
       " 0.29411765933036804,\n",
       " 0.25857841968536377,\n",
       " 0.25735294818878174,\n",
       " 0.281862735748291,\n",
       " 0.281862735748291,\n",
       " 0.2769607901573181,\n",
       " 0.27450981736183167,\n",
       " 0.27450981736183167,\n",
       " 0.27941176295280457,\n",
       " 0.26838234066963196,\n",
       " 0.2671568691730499,\n",
       " 0.2757352888584137,\n",
       " 0.2965686321258545,\n",
       " 0.27328431606292725,\n",
       " 0.2671568691730499,\n",
       " 0.2953431308269501,\n",
       " 0.2720588147640228,\n",
       " 0.23774509131908417,\n",
       " 0.26225489377975464,\n",
       " 0.2965686321258545,\n",
       " 0.25245097279548645,\n",
       " 0.2708333432674408,\n",
       " 0.2647058963775635,\n",
       " 0.25245097279548645,\n",
       " 0.27328431606292725,\n",
       " 0.2708333432674408,\n",
       " 0.281862735748291,\n",
       " 0.2855392098426819,\n",
       " 0.2977941036224365,\n",
       " 0.25735294818878174,\n",
       " 0.2549019753932953,\n",
       " 0.2659313678741455,\n",
       " 0.2928921580314636,\n",
       " 0.26348039507865906,\n",
       " 0.28308823704719543,\n",
       " 0.23161764442920685,\n",
       " 0.26225489377975464,\n",
       " 0.280637264251709,\n",
       " 0.27450981736183167,\n",
       " 0.2598039209842682,\n",
       " 0.27328431606292725,\n",
       " 0.24264705181121826,\n",
       " 0.26838234066963196,\n",
       " 0.26225489377975464,\n",
       " 0.2977941036224365,\n",
       " 0.2928921580314636,\n",
       " 0.27450981736183167,\n",
       " 0.27941176295280457,\n",
       " 0.2549019753932953,\n",
       " 0.25367647409439087,\n",
       " 0.2708333432674408,\n",
       " 0.28431373834609985,\n",
       " 0.30392158031463623,\n",
       " 0.25735294818878174,\n",
       " 0.25735294818878174,\n",
       " 0.2867647111415863,\n",
       " 0.2549019753932953,\n",
       " 0.26348039507865906,\n",
       " 0.2389705926179886,\n",
       " 0.2708333432674408,\n",
       " 0.24264705181121826,\n",
       " 0.28308823704719543,\n",
       " 0.24754901230335236,\n",
       " 0.28431373834609985,\n",
       " 0.28921568393707275,\n",
       " 0.25735294818878174,\n",
       " 0.26838234066963196,\n",
       " 0.23039215803146362,\n",
       " 0.2720588147640228,\n",
       " 0.25367647409439087,\n",
       " 0.27450981736183167,\n",
       " 0.2953431308269501,\n",
       " 0.2450980395078659,\n",
       " 0.3014705777168274,\n",
       " 0.2769607901573181,\n",
       " 0.2977941036224365,\n",
       " 0.26348039507865906,\n",
       " 0.2549019753932953,\n",
       " 0.2977941036224365,\n",
       " 0.26838234066963196,\n",
       " 0.2855392098426819,\n",
       " 0.2953431308269501,\n",
       " 0.2512255012989044,\n",
       " 0.2757352888584137,\n",
       " 0.28431373834609985,\n",
       " 0.2647058963775635,\n",
       " 0.2708333432674408,\n",
       " 0.23529411852359772,\n",
       " 0.2671568691730499,\n",
       " 0.3014705777168274,\n",
       " 0.2561274468898773,\n",
       " 0.30024510622024536,\n",
       " 0.2965686321258545,\n",
       " 0.2769607901573181,\n",
       " 0.27818626165390015,\n",
       " 0.23529411852359772,\n",
       " 0.2696078419685364,\n",
       " 0.28431373834609985,\n",
       " 0.3026960790157318,\n",
       " 0.2696078419685364,\n",
       " 0.27818626165390015,\n",
       " 0.28308823704719543,\n",
       " 0.2757352888584137,\n",
       " 0.3112744987010956,\n",
       " 0.28921568393707275,\n",
       " 0.2708333432674408,\n",
       " 0.2757352888584137,\n",
       " 0.2867647111415863,\n",
       " 0.26348039507865906,\n",
       " 0.29901960492134094,\n",
       " 0.26348039507865906,\n",
       " 0.2928921580314636,\n",
       " 0.2953431308269501,\n",
       " 0.25857841968536377,\n",
       " 0.2598039209842682,\n",
       " 0.2916666567325592,\n",
       " 0.280637264251709,\n",
       " 0.24264705181121826,\n",
       " 0.26225489377975464,\n",
       " 0.24632352590560913,\n",
       " 0.24877451360225677,\n",
       " 0.2561274468898773,\n",
       " 0.28431373834609985,\n",
       " 0.27818626165390015,\n",
       " 0.25735294818878174,\n",
       " 0.27941176295280457,\n",
       " 0.2867647111415863,\n",
       " 0.25857841968536377,\n",
       " 0.2904411852359772,\n",
       " 0.26348039507865906,\n",
       " 0.24264705181121826,\n",
       " 0.2867647111415863,\n",
       " 0.2696078419685364,\n",
       " 0.2916666567325592,\n",
       " 0.25245097279548645,\n",
       " 0.28308823704719543,\n",
       " 0.2696078419685364,\n",
       " 0.2867647111415863,\n",
       " 0.26838234066963196,\n",
       " 0.31985294818878174,\n",
       " 0.24264705181121826,\n",
       " 0.2977941036224365,\n",
       " 0.2389705926179886,\n",
       " 0.30392158031463623,\n",
       " 0.2696078419685364,\n",
       " 0.2708333432674408,\n",
       " 0.2512255012989044,\n",
       " 0.31985294818878174,\n",
       " 0.24754901230335236,\n",
       " 0.2720588147640228,\n",
       " 0.26838234066963196,\n",
       " 0.3186274468898773,\n",
       " 0.2708333432674408,\n",
       " 0.27328431606292725,\n",
       " 0.2549019753932953,\n",
       " 0.2855392098426819,\n",
       " 0.23529411852359772,\n",
       " 0.29411765933036804,\n",
       " 0.2659313678741455,\n",
       " 0.2561274468898773,\n",
       " 0.2916666567325592,\n",
       " 0.31004902720451355,\n",
       " 0.28308823704719543,\n",
       " 0.280637264251709,\n",
       " 0.26225489377975464,\n",
       " 0.2696078419685364,\n",
       " 0.26838234066963196,\n",
       " 0.2769607901573181,\n",
       " 0.2720588147640228,\n",
       " 0.2659313678741455,\n",
       " 0.27818626165390015,\n",
       " 0.2965686321258545,\n",
       " 0.25245097279548645,\n",
       " 0.28921568393707275,\n",
       " 0.2904411852359772,\n",
       " 0.28308823704719543,\n",
       " 0.2855392098426819,\n",
       " 0.26225489377975464,\n",
       " 0.27328431606292725,\n",
       " 0.28431373834609985,\n",
       " 0.27328431606292725,\n",
       " 0.3063725531101227,\n",
       " 0.25735294818878174,\n",
       " 0.25735294818878174,\n",
       " 0.26225489377975464,\n",
       " 0.27328431606292725,\n",
       " 0.29901960492134094,\n",
       " 0.29411765933036804,\n",
       " 0.26225489377975464,\n",
       " 0.28431373834609985,\n",
       " 0.2965686321258545,\n",
       " 0.3137255012989044,\n",
       " 0.2291666716337204,\n",
       " 0.24754901230335236,\n",
       " 0.24877451360225677,\n",
       " 0.25245097279548645,\n",
       " 0.28799018263816833,\n",
       " 0.25857841968536377,\n",
       " 0.2671568691730499,\n",
       " 0.2904411852359772,\n",
       " 0.2389705926179886,\n",
       " 0.2855392098426819,\n",
       " 0.27328431606292725,\n",
       " 0.28431373834609985,\n",
       " 0.26838234066963196,\n",
       " 0.23651960492134094,\n",
       " 0.27328431606292725,\n",
       " 0.2659313678741455,\n",
       " 0.28431373834609985,\n",
       " 0.2598039209842682,\n",
       " 0.2757352888584137,\n",
       " 0.29411765933036804,\n",
       " 0.25735294818878174,\n",
       " 0.30024510622024536,\n",
       " 0.2855392098426819,\n",
       " 0.28799018263816833,\n",
       " 0.2904411852359772,\n",
       " 0.28308823704719543,\n",
       " 0.29411765933036804,\n",
       " 0.2671568691730499,\n",
       " 0.2757352888584137,\n",
       " 0.280637264251709,\n",
       " 0.27818626165390015,\n",
       " 0.26348039507865906,\n",
       " 0.2696078419685364,\n",
       " 0.2965686321258545,\n",
       " 0.26838234066963196,\n",
       " 0.26838234066963196,\n",
       " 0.2708333432674408,\n",
       " 0.28799018263816833,\n",
       " 0.28308823704719543,\n",
       " 0.2720588147640228,\n",
       " 0.2389705926179886,\n",
       " 0.2757352888584137,\n",
       " 0.30882352590560913,\n",
       " 0.2401960790157318,\n",
       " 0.280637264251709,\n",
       " 0.280637264251709,\n",
       " 0.24877451360225677,\n",
       " 0.25,\n",
       " 0.2598039209842682,\n",
       " 0.2242647111415863,\n",
       " 0.2928921580314636,\n",
       " 0.281862735748291,\n",
       " 0.24387255311012268,\n",
       " 0.3014705777168274,\n",
       " 0.26225489377975464,\n",
       " 0.2867647111415863,\n",
       " 0.30882352590560913,\n",
       " 0.27450981736183167,\n",
       " 0.27450981736183167,\n",
       " 0.2977941036224365,\n",
       " 0.2659313678741455,\n",
       " 0.28921568393707275,\n",
       " 0.25857841968536377,\n",
       " 0.24754901230335236,\n",
       " 0.32598039507865906,\n",
       " 0.27941176295280457,\n",
       " 0.2450980395078659,\n",
       " 0.3112744987010956,\n",
       " 0.2757352888584137,\n",
       " 0.2659313678741455,\n",
       " 0.28431373834609985,\n",
       " 0.27328431606292725,\n",
       " 0.27818626165390015,\n",
       " 0.2561274468898773,\n",
       " 0.2696078419685364,\n",
       " 0.24387255311012268,\n",
       " 0.28431373834609985,\n",
       " 0.2659313678741455,\n",
       " 0.2561274468898773,\n",
       " 0.27328431606292725,\n",
       " 0.24754901230335236,\n",
       " 0.2953431308269501,\n",
       " 0.2389705926179886,\n",
       " 0.29901960492134094,\n",
       " 0.28799018263816833,\n",
       " 0.2512255012989044,\n",
       " 0.26838234066963196,\n",
       " 0.2769607901573181,\n",
       " 0.2659313678741455,\n",
       " 0.28921568393707275,\n",
       " 0.280637264251709,\n",
       " 0.26225489377975464,\n",
       " 0.2598039209842682,\n",
       " 0.2855392098426819,\n",
       " 0.2720588147640228,\n",
       " 0.27328431606292725,\n",
       " 0.25245097279548645,\n",
       " 0.2855392098426819,\n",
       " 0.30514705181121826,\n",
       " 0.25245097279548645,\n",
       " 0.27818626165390015,\n",
       " 0.2549019753932953,\n",
       " 0.27941176295280457,\n",
       " 0.2928921580314636,\n",
       " 0.27450981736183167,\n",
       " 0.2647058963775635,\n",
       " 0.23651960492134094,\n",
       " 0.2708333432674408,\n",
       " 0.2610294222831726,\n",
       " 0.27818626165390015,\n",
       " 0.2708333432674408,\n",
       " 0.29411765933036804,\n",
       " 0.26348039507865906,\n",
       " 0.2549019753932953,\n",
       " 0.281862735748291,\n",
       " 0.26838234066963196,\n",
       " 0.1973039209842682,\n",
       " 0.3174019753932953,\n",
       " 0.2757352888584137,\n",
       " 0.31617647409439087,\n",
       " 0.2757352888584137,\n",
       " 0.27818626165390015,\n",
       " 0.2561274468898773,\n",
       " 0.281862735748291,\n",
       " 0.27450981736183167,\n",
       " 0.2671568691730499,\n",
       " 0.2720588147640228,\n",
       " 0.2561274468898773,\n",
       " 0.2561274468898773,\n",
       " 0.31004902720451355,\n",
       " 0.2708333432674408,\n",
       " 0.281862735748291,\n",
       " 0.26838234066963196,\n",
       " 0.2953431308269501,\n",
       " 0.26838234066963196,\n",
       " 0.2720588147640228,\n",
       " 0.2904411852359772,\n",
       " 0.25735294818878174,\n",
       " 0.2696078419685364,\n",
       " 0.2904411852359772,\n",
       " 0.29901960492134094,\n",
       " 0.26838234066963196,\n",
       " 0.2561274468898773,\n",
       " 0.26225489377975464,\n",
       " 0.30392158031463623,\n",
       " 0.25,\n",
       " 0.28921568393707275,\n",
       " 0.26838234066963196,\n",
       " 0.2977941036224365,\n",
       " 0.25735294818878174,\n",
       " 0.2708333432674408,\n",
       " 0.3063725531101227,\n",
       " 0.27450981736183167,\n",
       " 0.2720588147640228,\n",
       " 0.27450981736183167,\n",
       " 0.2916666567325592,\n",
       " 0.23651960492134094,\n",
       " 0.25857841968536377,\n",
       " 0.2769607901573181,\n",
       " 0.24387255311012268,\n",
       " 0.26348039507865906,\n",
       " 0.2965686321258545,\n",
       " 0.2598039209842682,\n",
       " 0.2549019753932953,\n",
       " 0.280637264251709,\n",
       " 0.26348039507865906,\n",
       " 0.30882352590560913,\n",
       " 0.2867647111415863,\n",
       " 0.26225489377975464,\n",
       " 0.25857841968536377,\n",
       " 0.2977941036224365,\n",
       " 0.2647058963775635,\n",
       " 0.2659313678741455,\n",
       " 0.2916666567325592,\n",
       " 0.24264705181121826,\n",
       " 0.26838234066963196,\n",
       " 0.2610294222831726,\n",
       " 0.280637264251709,\n",
       " 0.23161764442920685,\n",
       " 0.2757352888584137,\n",
       " 0.26225489377975464,\n",
       " 0.2769607901573181]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1902438998222351,\n",
       " 0.10731707513332367,\n",
       " 0.10731707513332367,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.06341463327407837,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.06341463327407837,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.06341463327407837,\n",
       " 0.20975609123706818,\n",
       " 0.10731707513332367,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.06341463327407837,\n",
       " 0.4292683005332947,\n",
       " 0.06341463327407837,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.06341463327407837,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.10731707513332367,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.06341463327407837,\n",
       " 0.20975609123706818,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.06341463327407837,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.10731707513332367,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.10731707513332367,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.06341463327407837,\n",
       " 0.06341463327407837,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.10731707513332367,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.10731707513332367,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.06341463327407837,\n",
       " 0.10731707513332367,\n",
       " 0.20975609123706818,\n",
       " 0.10731707513332367,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.06341463327407837,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.10731707513332367,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.06341463327407837,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.06341463327407837,\n",
       " 0.1902438998222351,\n",
       " 0.10731707513332367,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.06341463327407837,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.10731707513332367,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.06341463327407837,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.06341463327407837,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.06341463327407837,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.10731707513332367,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.10731707513332367,\n",
       " 0.10731707513332367,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.06341463327407837,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.06341463327407837,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.06341463327407837,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.06341463327407837,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.06341463327407837,\n",
       " 0.06341463327407837,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.10731707513332367,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.06341463327407837,\n",
       " 0.20975609123706818,\n",
       " 0.10731707513332367,\n",
       " 0.06341463327407837,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.06341463327407837,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.10731707513332367,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.10731707513332367,\n",
       " 0.1902438998222351,\n",
       " 0.20975609123706818,\n",
       " 0.1902438998222351,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.1902438998222351,\n",
       " 0.10731707513332367,\n",
       " 0.4292683005332947,\n",
       " 0.20975609123706818,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947,\n",
       " 0.4292683005332947]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[554252224.0,\n",
       " 309166048.0,\n",
       " 157752992.0,\n",
       " 52692396.0,\n",
       " 8596191.0,\n",
       " 4249935.0,\n",
       " 3580297.75,\n",
       " 2509741.25,\n",
       " 2409274.75,\n",
       " 1240732.5,\n",
       " 2318705.0,\n",
       " 2230688.25,\n",
       " 2346646.75,\n",
       " 2798000.0,\n",
       " 1570417.375,\n",
       " 2091961.25,\n",
       " 2505890.25,\n",
       " 2588920.75,\n",
       " 1844366.75,\n",
       " 1370857.0,\n",
       " 1798683.125,\n",
       " 1852906.875,\n",
       " 3257242.0,\n",
       " 2681166.0,\n",
       " 2347663.75,\n",
       " 2902397.25,\n",
       " 1944190.75,\n",
       " 2971815.75,\n",
       " 2208794.25,\n",
       " 2743939.75,\n",
       " 2582825.0,\n",
       " 2741575.5,\n",
       " 1834714.375,\n",
       " 2189595.25,\n",
       " 2244626.25,\n",
       " 2394275.0,\n",
       " 1662795.625,\n",
       " 1739166.125,\n",
       " 2755085.5,\n",
       " 1732194.0,\n",
       " 1659844.375,\n",
       " 1934613.625,\n",
       " 2672938.75,\n",
       " 2870531.25,\n",
       " 1942301.0,\n",
       " 2848179.25,\n",
       " 3007393.5,\n",
       " 2140623.5,\n",
       " 1698368.5,\n",
       " 1969749.125,\n",
       " 1934239.875,\n",
       " 2930514.0,\n",
       " 2186756.5,\n",
       " 1396188.875,\n",
       " 1681052.75,\n",
       " 1719035.75,\n",
       " 1548250.25,\n",
       " 2856175.75,\n",
       " 2241401.0,\n",
       " 1786267.25,\n",
       " 1902041.0,\n",
       " 1525347.875,\n",
       " 1763894.625,\n",
       " 1697039.5,\n",
       " 1576744.75,\n",
       " 2215438.0,\n",
       " 1992666.0,\n",
       " 3118371.75,\n",
       " 2174076.5,\n",
       " 1208287.75,\n",
       " 1713501.375,\n",
       " 1339867.0,\n",
       " 2064724.5,\n",
       " 1338854.875,\n",
       " 1657441.0,\n",
       " 1042220.6875,\n",
       " 1374863.625,\n",
       " 1954346.5,\n",
       " 1632360.625,\n",
       " 2953568.25,\n",
       " 1862814.375,\n",
       " 1601326.125,\n",
       " 1444548.875,\n",
       " 1989634.5,\n",
       " 1283037.125,\n",
       " 1540642.0,\n",
       " 1798897.0,\n",
       " 1679384.375,\n",
       " 1451782.875,\n",
       " 2309044.75,\n",
       " 2052295.0,\n",
       " 1430611.125,\n",
       " 1355785.0,\n",
       " 1463359.875,\n",
       " 899345.0,\n",
       " 1606470.375,\n",
       " 1950457.375,\n",
       " 1311605.625,\n",
       " 1020841.9375,\n",
       " 1173587.75,\n",
       " 1144287.25,\n",
       " 1562706.375,\n",
       " 1253074.25,\n",
       " 981604.375,\n",
       " 1242024.5,\n",
       " 1695833.875,\n",
       " 1841480.0,\n",
       " 1393928.375,\n",
       " 1473769.375,\n",
       " 1378807.375,\n",
       " 1215400.125,\n",
       " 1496785.25,\n",
       " 2427255.25,\n",
       " 1919448.375,\n",
       " 961008.5,\n",
       " 1989161.875,\n",
       " 1476405.125,\n",
       " 1533931.0,\n",
       " 1617571.875,\n",
       " 1388779.875,\n",
       " 1595767.875,\n",
       " 1162030.75,\n",
       " 1258090.0,\n",
       " 1491584.0,\n",
       " 2038067.75,\n",
       " 1176935.75,\n",
       " 764984.0625,\n",
       " 972177.5625,\n",
       " 953174.25,\n",
       " 1317446.375,\n",
       " 1566852.25,\n",
       " 1190907.0,\n",
       " 1645629.125,\n",
       " 1607710.125,\n",
       " 1266445.5,\n",
       " 893608.6875,\n",
       " 923473.3125,\n",
       " 839572.625,\n",
       " 1417848.75,\n",
       " 1174949.25,\n",
       " 1387536.375,\n",
       " 1390281.125,\n",
       " 2108374.25,\n",
       " 1972470.875,\n",
       " 1017101.1875,\n",
       " 1489533.5,\n",
       " 1620019.875,\n",
       " 900212.5,\n",
       " 1133171.875,\n",
       " 977864.25,\n",
       " 946877.8125,\n",
       " 1088652.875,\n",
       " 1015023.4375,\n",
       " 821035.1875,\n",
       " 1002208.3125,\n",
       " 643034.4375,\n",
       " 1118064.125,\n",
       " 905360.9375,\n",
       " 982816.25,\n",
       " 1248565.375,\n",
       " 935897.25,\n",
       " 1031824.9375,\n",
       " 838065.75,\n",
       " 913758.5625,\n",
       " 710378.875,\n",
       " 1403401.25,\n",
       " 1592854.875,\n",
       " 1202024.5,\n",
       " 1672410.375,\n",
       " 1132295.25,\n",
       " 977764.5625,\n",
       " 687051.6875,\n",
       " 1023125.75,\n",
       " 1390524.5,\n",
       " 1141145.625,\n",
       " 1672631.625,\n",
       " 1559178.375,\n",
       " 897853.1875,\n",
       " 872346.75,\n",
       " 814506.8125,\n",
       " 897452.5625,\n",
       " 1240515.5,\n",
       " 1713278.375,\n",
       " 743349.25,\n",
       " 877901.8125,\n",
       " 1152309.25,\n",
       " 1044948.625,\n",
       " 978957.4375,\n",
       " 1135806.5,\n",
       " 1222122.625,\n",
       " 1061060.375,\n",
       " 1218431.875,\n",
       " 1622443.0,\n",
       " 1349358.375,\n",
       " 1040695.5,\n",
       " 750586.875,\n",
       " 1063798.375,\n",
       " 1163364.75,\n",
       " 1672763.875,\n",
       " 1149056.75,\n",
       " 1057535.125,\n",
       " 1175858.875,\n",
       " 1214881.75,\n",
       " 921342.0625,\n",
       " 472985.90625,\n",
       " 774476.8125,\n",
       " 884998.75,\n",
       " 943547.125,\n",
       " 658965.875,\n",
       " 682463.1875,\n",
       " 760791.6875,\n",
       " 921981.625,\n",
       " 870876.375,\n",
       " 672419.1875,\n",
       " 668683.125,\n",
       " 877583.5,\n",
       " 677642.75,\n",
       " 650929.375,\n",
       " 818862.875,\n",
       " 698885.0,\n",
       " 722452.9375,\n",
       " 1298950.5,\n",
       " 870684.9375,\n",
       " 968759.75,\n",
       " 549032.0625,\n",
       " 754083.5,\n",
       " 932900.3125,\n",
       " 798972.1875,\n",
       " 743624.9375,\n",
       " 785334.5625,\n",
       " 582443.0,\n",
       " 1040401.4375,\n",
       " 729648.0625,\n",
       " 652564.1875,\n",
       " 1282955.625,\n",
       " 1483304.0,\n",
       " 1446408.375,\n",
       " 928456.5625,\n",
       " 512606.1875,\n",
       " 961632.625,\n",
       " 883898.125,\n",
       " 724642.875,\n",
       " 594561.3125,\n",
       " 805327.3125,\n",
       " 519243.65625,\n",
       " 590720.5625,\n",
       " 770056.9375,\n",
       " 883817.625,\n",
       " 757605.625,\n",
       " 901923.3125,\n",
       " 593544.375,\n",
       " 599333.8125,\n",
       " 592628.3125,\n",
       " 685107.75,\n",
       " 682802.875,\n",
       " 947100.875,\n",
       " 1309479.0,\n",
       " 973816.8125,\n",
       " 721624.625,\n",
       " 562774.8125,\n",
       " 582842.125,\n",
       " 496945.75,\n",
       " 504495.5,\n",
       " 585716.1875,\n",
       " 824934.0625,\n",
       " 660619.125,\n",
       " 663144.9375,\n",
       " 680453.9375,\n",
       " 898986.75,\n",
       " 822704.1875,\n",
       " 686864.0,\n",
       " 880605.4375,\n",
       " 921176.5625,\n",
       " 873702.125,\n",
       " 805429.875,\n",
       " 565087.3125,\n",
       " 519362.5,\n",
       " 688595.3125,\n",
       " 498032.25,\n",
       " 538444.875,\n",
       " 504053.0625,\n",
       " 511835.6875,\n",
       " 982730.375,\n",
       " 1119391.125,\n",
       " 641126.75,\n",
       " 688834.5,\n",
       " 541919.0,\n",
       " 417916.625,\n",
       " 836781.3125,\n",
       " 648524.5625,\n",
       " 570775.0625,\n",
       " 693312.8125,\n",
       " 864500.625,\n",
       " 975600.9375,\n",
       " 1049914.25,\n",
       " 703853.5,\n",
       " 772573.625,\n",
       " 623478.8125,\n",
       " 445444.5625,\n",
       " 679825.875,\n",
       " 578184.0625,\n",
       " 403757.90625,\n",
       " 588130.1875,\n",
       " 538623.75,\n",
       " 720082.125,\n",
       " 884747.6875,\n",
       " 540687.625,\n",
       " 615543.0,\n",
       " 803240.25,\n",
       " 521186.25,\n",
       " 909102.125,\n",
       " 647603.625,\n",
       " 534105.5,\n",
       " 481082.28125,\n",
       " 409755.34375,\n",
       " 779247.8125,\n",
       " 670601.75,\n",
       " 552945.0625,\n",
       " 281652.75,\n",
       " 570244.375,\n",
       " 723850.75,\n",
       " 637911.1875,\n",
       " 285641.75,\n",
       " 498655.84375,\n",
       " 488882.96875,\n",
       " 474187.4375,\n",
       " 626937.8125,\n",
       " 488767.28125,\n",
       " 507648.4375,\n",
       " 641642.9375,\n",
       " 577622.5,\n",
       " 450062.5,\n",
       " 406711.96875,\n",
       " 511590.09375,\n",
       " 756321.9375,\n",
       " 727768.0625,\n",
       " 617346.125,\n",
       " 796198.0625,\n",
       " 687878.6875,\n",
       " 404313.8125,\n",
       " 375287.53125,\n",
       " 418385.40625,\n",
       " 659763.8125,\n",
       " 622072.6875,\n",
       " 645241.125,\n",
       " 412543.125,\n",
       " 503870.75,\n",
       " 523517.8125,\n",
       " 440727.09375,\n",
       " 326623.8125,\n",
       " 407410.0,\n",
       " 481296.75,\n",
       " 517454.1875,\n",
       " 513094.1875,\n",
       " 611013.625,\n",
       " 827224.1875,\n",
       " 831607.9375,\n",
       " 444072.71875,\n",
       " 623691.8125,\n",
       " 449067.84375,\n",
       " 465084.5625,\n",
       " 467804.8125,\n",
       " 325337.75,\n",
       " 387603.71875,\n",
       " 430323.375,\n",
       " 531554.1875,\n",
       " 468283.84375,\n",
       " 725001.625,\n",
       " 993755.75,\n",
       " 378987.21875,\n",
       " 509058.1875,\n",
       " 308689.5625,\n",
       " 527621.375,\n",
       " 560220.875,\n",
       " 444671.28125,\n",
       " 288198.34375,\n",
       " 513346.65625,\n",
       " 433072.1875,\n",
       " 501342.65625,\n",
       " 352335.375,\n",
       " 376257.28125,\n",
       " 618148.5625,\n",
       " 657878.8125,\n",
       " 433964.96875,\n",
       " 506337.09375,\n",
       " 492561.59375,\n",
       " 542753.5,\n",
       " 236771.796875,\n",
       " 282110.28125,\n",
       " 619717.875,\n",
       " 479522.71875,\n",
       " 374697.4375,\n",
       " 744144.5625,\n",
       " 815134.5,\n",
       " 767459.375,\n",
       " 466343.09375,\n",
       " 225202.09375,\n",
       " 331976.59375,\n",
       " 236594.703125,\n",
       " 327981.875,\n",
       " 231779.375,\n",
       " 430669.65625,\n",
       " 585434.375,\n",
       " 452660.5,\n",
       " 505180.0,\n",
       " 406719.4375,\n",
       " 289966.5625,\n",
       " 453693.75,\n",
       " 253081.765625,\n",
       " 339312.34375,\n",
       " 310571.0625,\n",
       " 530499.75,\n",
       " 238460.140625,\n",
       " 203384.875,\n",
       " 346921.40625,\n",
       " 324868.8125,\n",
       " 252610.28125,\n",
       " 572349.0625,\n",
       " 615295.875,\n",
       " 357423.03125,\n",
       " 350787.40625,\n",
       " 455025.75,\n",
       " 220216.90625,\n",
       " 333755.5625,\n",
       " 284034.09375,\n",
       " 327491.0625,\n",
       " 170573.796875,\n",
       " 161703.3125,\n",
       " 263976.875,\n",
       " 289510.40625,\n",
       " 652711.9375,\n",
       " 295297.03125,\n",
       " 249241.625,\n",
       " 331747.4375,\n",
       " 365284.625,\n",
       " 605339.875,\n",
       " 233687.390625,\n",
       " 183533.140625,\n",
       " 252993.859375,\n",
       " 293445.5625,\n",
       " 212866.21875,\n",
       " 215170.703125,\n",
       " 221375.0625,\n",
       " 171496.296875,\n",
       " 262797.09375,\n",
       " 327347.9375,\n",
       " 330616.90625,\n",
       " 286410.84375,\n",
       " 274688.125,\n",
       " 258928.0,\n",
       " 485702.8125,\n",
       " 391620.4375,\n",
       " 150977.890625,\n",
       " 206717.78125,\n",
       " 158334.375,\n",
       " 463432.90625,\n",
       " 403605.59375,\n",
       " 369120.34375,\n",
       " 446719.40625,\n",
       " 362340.96875,\n",
       " 258085.375,\n",
       " 376924.59375,\n",
       " 231773.859375,\n",
       " 345754.09375,\n",
       " 334965.96875,\n",
       " 265575.375,\n",
       " 340629.09375,\n",
       " 244016.296875,\n",
       " 147056.21875,\n",
       " 212548.984375,\n",
       " 265669.125,\n",
       " 285514.53125,\n",
       " 203564.203125,\n",
       " 324530.1875,\n",
       " 256983.21875,\n",
       " 300415.3125,\n",
       " 234524.375,\n",
       " 200783.984375,\n",
       " 180572.09375,\n",
       " 317303.59375,\n",
       " 314592.46875,\n",
       " 233189.171875,\n",
       " 252400.9375,\n",
       " 325408.625,\n",
       " 223425.0625,\n",
       " 214112.71875,\n",
       " 238677.859375,\n",
       " 325855.6875,\n",
       " 204638.859375,\n",
       " 214495.546875,\n",
       " 255385.921875,\n",
       " 161299.015625,\n",
       " 237285.84375,\n",
       " 411590.65625,\n",
       " 490875.65625,\n",
       " 299748.71875,\n",
       " 309008.15625,\n",
       " 128315.65625,\n",
       " 116158.125,\n",
       " 132112.640625]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[418016032.0,\n",
       " 228648176.0,\n",
       " 102466432.0,\n",
       " 16075115.0,\n",
       " 5058272.5,\n",
       " 4025411.5,\n",
       " 3212872.5,\n",
       " 2554650.0,\n",
       " 2965114.0,\n",
       " 1679631.5,\n",
       " 1246108.25,\n",
       " 2848399.5,\n",
       " 3437724.0,\n",
       " 1888893.5,\n",
       " 2830099.75,\n",
       " 1735826.75,\n",
       " 4024075.0,\n",
       " 1661618.875,\n",
       " 1975692.75,\n",
       " 980017.6875,\n",
       " 1244852.875,\n",
       " 869115.6875,\n",
       " 3946762.5,\n",
       " 972146.875,\n",
       " 1323445.75,\n",
       " 1129408.125,\n",
       " 1726557.0,\n",
       " 3372362.25,\n",
       " 2891357.25,\n",
       " 2328081.75,\n",
       " 2391714.25,\n",
       " 1732796.625,\n",
       " 1369856.5,\n",
       " 3680454.25,\n",
       " 1195717.125,\n",
       " 3090683.25,\n",
       " 2936978.0,\n",
       " 1472477.625,\n",
       " 2438010.75,\n",
       " 2477316.0,\n",
       " 1048570.0,\n",
       " 4151045.25,\n",
       " 2620845.5,\n",
       " 2125951.75,\n",
       " 1779555.625,\n",
       " 3397684.5,\n",
       " 2194389.5,\n",
       " 1892542.25,\n",
       " 1052719.5,\n",
       " 794817.6875,\n",
       " 1183553.5,\n",
       " 1779839.125,\n",
       " 1133941.25,\n",
       " 1654689.125,\n",
       " 1799089.125,\n",
       " 1511536.75,\n",
       " 2785606.5,\n",
       " 3665147.25,\n",
       " 1206745.125,\n",
       " 2090485.125,\n",
       " 1479644.125,\n",
       " 1758777.875,\n",
       " 2697546.5,\n",
       " 1190632.5,\n",
       " 1973249.5,\n",
       " 1280587.5,\n",
       " 985890.8125,\n",
       " 3452803.5,\n",
       " 1651734.75,\n",
       " 1932141.625,\n",
       " 1549688.875,\n",
       " 2249499.5,\n",
       " 1534017.5,\n",
       " 1070965.75,\n",
       " 1311307.25,\n",
       " 797385.125,\n",
       " 2474060.75,\n",
       " 2294103.0,\n",
       " 2187798.0,\n",
       " 2255138.75,\n",
       " 1929108.0,\n",
       " 1675470.625,\n",
       " 2850998.75,\n",
       " 2371764.5,\n",
       " 202790.40625,\n",
       " 1239486.5,\n",
       " 2212932.25,\n",
       " 1712290.125,\n",
       " 3109123.5,\n",
       " 1896803.875,\n",
       " 830792.6875,\n",
       " 415551.15625,\n",
       " 1536086.625,\n",
       " 950360.5,\n",
       " 1416340.0,\n",
       " 1334384.125,\n",
       " 2061916.75,\n",
       " 648822.6875,\n",
       " 682337.3125,\n",
       " 843950.75,\n",
       " 1664806.75,\n",
       " 1421653.875,\n",
       " 1429051.75,\n",
       " 653256.1875,\n",
       " 1415994.5,\n",
       " 934520.875,\n",
       " 773193.4375,\n",
       " 2491705.0,\n",
       " 1438366.25,\n",
       " 464655.84375,\n",
       " 1211500.75,\n",
       " 2079501.625,\n",
       " 2999065.25,\n",
       " 431354.9375,\n",
       " 1063365.25,\n",
       " 1824936.125,\n",
       " 1512717.375,\n",
       " 2882552.25,\n",
       " 1793407.75,\n",
       " 912750.3125,\n",
       " 1025047.1875,\n",
       " 727593.0625,\n",
       " 1697145.5,\n",
       " 1475521.875,\n",
       " 1140363.375,\n",
       " 1724130.625,\n",
       " 935391.0,\n",
       " 1181749.5,\n",
       " 641613.5625,\n",
       " 2041550.375,\n",
       " 1910775.625,\n",
       " 1058082.0,\n",
       " 1659924.5,\n",
       " 1363926.125,\n",
       " 1979477.125,\n",
       " 399399.03125,\n",
       " 899591.625,\n",
       " 452953.90625,\n",
       " 2339666.0,\n",
       " 1304952.375,\n",
       " 1466649.0,\n",
       " 2888958.25,\n",
       " 1669916.875,\n",
       " 695255.3125,\n",
       " 612828.875,\n",
       " 418648.8125,\n",
       " 1694374.75,\n",
       " 511096.65625,\n",
       " 1439427.125,\n",
       " 925998.625,\n",
       " 918838.3125,\n",
       " 1868116.75,\n",
       " 717934.9375,\n",
       " 683869.375,\n",
       " 722959.6875,\n",
       " 398057.28125,\n",
       " 1865181.375,\n",
       " 413191.34375,\n",
       " 1261343.875,\n",
       " 857020.625,\n",
       " 575438.75,\n",
       " 957212.3125,\n",
       " 1383553.5,\n",
       " 910925.5625,\n",
       " 1015532.3125,\n",
       " 1509487.125,\n",
       " 2547594.25,\n",
       " 2458207.5,\n",
       " 940081.8125,\n",
       " 2597171.5,\n",
       " 579285.3125,\n",
       " 398321.21875,\n",
       " 520706.6875,\n",
       " 1602102.125,\n",
       " 1577342.0,\n",
       " 2757778.5,\n",
       " 1303738.375,\n",
       " 559042.25,\n",
       " 1094118.0,\n",
       " 662209.0625,\n",
       " 799479.125,\n",
       " 1377883.75,\n",
       " 1116233.125,\n",
       " 372573.75,\n",
       " 1258185.375,\n",
       " 902906.0625,\n",
       " 664161.375,\n",
       " 621672.0,\n",
       " 694623.625,\n",
       " 897906.5,\n",
       " 667434.375,\n",
       " 2614693.5,\n",
       " 1514237.25,\n",
       " 1506814.25,\n",
       " 1062289.375,\n",
       " 830688.3125,\n",
       " 1140965.875,\n",
       " 1930677.75,\n",
       " 1330396.875,\n",
       " 1377261.875,\n",
       " 935142.0625,\n",
       " 1773264.125,\n",
       " 1817556.0,\n",
       " 768111.0625,\n",
       " 892095.875,\n",
       " 1742323.375,\n",
       " 1168478.75,\n",
       " 1091195.125,\n",
       " 790678.1875,\n",
       " 841961.9375,\n",
       " 1860270.625,\n",
       " 582589.5,\n",
       " 378988.4375,\n",
       " 879034.375,\n",
       " 1177734.25,\n",
       " 1066946.125,\n",
       " 595056.0625,\n",
       " 776936.5625,\n",
       " 488537.4375,\n",
       " 970134.25,\n",
       " 1029736.375,\n",
       " 986397.1875,\n",
       " 571673.625,\n",
       " 288576.15625,\n",
       " 1166510.875,\n",
       " 723478.375,\n",
       " 631589.0625,\n",
       " 1618844.125,\n",
       " 1529084.0,\n",
       " 689235.6875,\n",
       " 946548.625,\n",
       " 986360.6875,\n",
       " 634226.0,\n",
       " 803665.3125,\n",
       " 1165841.125,\n",
       " 799218.125,\n",
       " 1148192.5,\n",
       " 435027.4375,\n",
       " 527191.5625,\n",
       " 529526.9375,\n",
       " 1033114.4375,\n",
       " 604513.1875,\n",
       " 1158933.875,\n",
       " 649326.9375,\n",
       " 479592.78125,\n",
       " 760112.0625,\n",
       " 409174.0625,\n",
       " 694215.625,\n",
       " 885738.0,\n",
       " 1124100.0,\n",
       " 1184515.875,\n",
       " 595090.125,\n",
       " 452094.21875,\n",
       " 393024.6875,\n",
       " 849173.375,\n",
       " 1613133.25,\n",
       " 897623.4375,\n",
       " 843749.4375,\n",
       " 638147.0,\n",
       " 154588.484375,\n",
       " 495805.15625,\n",
       " 834306.8125,\n",
       " 803627.5625,\n",
       " 507005.78125,\n",
       " 608712.0625,\n",
       " 644749.3125,\n",
       " 633888.25,\n",
       " 1397482.5,\n",
       " 806421.9375,\n",
       " 672908.9375,\n",
       " 1064245.375,\n",
       " 916465.3125,\n",
       " 1570193.125,\n",
       " 679584.125,\n",
       " 733156.5,\n",
       " 169135.578125,\n",
       " 563639.5,\n",
       " 928287.875,\n",
       " 481413.59375,\n",
       " 525963.5625,\n",
       " 510478.09375,\n",
       " 509408.34375,\n",
       " 1760974.0,\n",
       " 610228.125,\n",
       " 831282.4375,\n",
       " 1357269.25,\n",
       " 578704.5625,\n",
       " 427780.65625,\n",
       " 985747.0625,\n",
       " 421284.8125,\n",
       " 764465.375,\n",
       " 583661.5625,\n",
       " 1166107.5,\n",
       " 1260450.875,\n",
       " 861885.375,\n",
       " 848264.75,\n",
       " 787542.0,\n",
       " 267852.28125,\n",
       " 1019466.625,\n",
       " 425690.375,\n",
       " 443725.65625,\n",
       " 613008.0625,\n",
       " 482363.5,\n",
       " 813082.6875,\n",
       " 1040369.8125,\n",
       " 505632.75,\n",
       " 765706.75,\n",
       " 466381.375,\n",
       " 595486.0625,\n",
       " 320269.125,\n",
       " 502353.9375,\n",
       " 334760.1875,\n",
       " 756691.25,\n",
       " 502073.375,\n",
       " 525620.125,\n",
       " 688864.4375,\n",
       " 339633.71875,\n",
       " 69197.9140625,\n",
       " 176665.671875,\n",
       " 905210.3125,\n",
       " 703408.625,\n",
       " 282525.21875,\n",
       " 443696.15625,\n",
       " 439209.875,\n",
       " 768544.375,\n",
       " 217420.234375,\n",
       " 399373.1875,\n",
       " 305788.46875,\n",
       " 635401.6875,\n",
       " 504707.28125,\n",
       " 820084.3125,\n",
       " 333649.15625,\n",
       " 601441.5,\n",
       " 619109.75,\n",
       " 629081.0,\n",
       " 564221.25,\n",
       " 646665.625,\n",
       " 910953.75,\n",
       " 473256.5,\n",
       " 741683.1875,\n",
       " 324771.53125,\n",
       " 301734.15625,\n",
       " 569735.625,\n",
       " 1350572.5,\n",
       " 143772.6875,\n",
       " 546352.375,\n",
       " 508827.0,\n",
       " 466773.625,\n",
       " 529236.0,\n",
       " 219267.75,\n",
       " 651136.4375,\n",
       " 413664.96875,\n",
       " 609839.25,\n",
       " 888175.4375,\n",
       " 750509.875,\n",
       " 1430473.625,\n",
       " 494327.96875,\n",
       " 532879.9375,\n",
       " 760938.375,\n",
       " 244548.5625,\n",
       " 239915.265625,\n",
       " 556938.6875,\n",
       " 156423.46875,\n",
       " 323357.46875,\n",
       " 157523.90625,\n",
       " 909242.0625,\n",
       " 244497.3125,\n",
       " 738858.0625,\n",
       " 760881.625,\n",
       " 502920.8125,\n",
       " 250368.640625,\n",
       " 244044.484375,\n",
       " 692295.75,\n",
       " 313167.0,\n",
       " 174176.53125,\n",
       " 179671.15625,\n",
       " 967308.75,\n",
       " 469240.78125,\n",
       " 310657.875,\n",
       " 303113.6875,\n",
       " 193065.1875,\n",
       " 604191.375,\n",
       " 361050.0,\n",
       " 633050.75,\n",
       " 367523.5,\n",
       " 722727.1875,\n",
       " 513068.53125,\n",
       " 234712.078125,\n",
       " 590062.625,\n",
       " 359692.0625,\n",
       " 505618.03125,\n",
       " 242547.6875,\n",
       " 1422380.625,\n",
       " 817226.25,\n",
       " 305269.0625,\n",
       " 572595.0625,\n",
       " 96907.3828125,\n",
       " 249840.234375,\n",
       " 337800.375,\n",
       " 166196.875,\n",
       " 322120.6875,\n",
       " 375915.46875,\n",
       " 224583.15625,\n",
       " 566288.875,\n",
       " 432964.6875,\n",
       " 192939.28125,\n",
       " 502839.71875,\n",
       " 100024.75,\n",
       " 297283.09375,\n",
       " 136135.109375,\n",
       " 324917.75,\n",
       " 316593.25,\n",
       " 96935.6796875,\n",
       " 264758.21875,\n",
       " 337900.65625,\n",
       " 186175.109375,\n",
       " 516380.1875,\n",
       " 178078.59375,\n",
       " 670608.9375,\n",
       " 288598.78125,\n",
       " 554955.4375,\n",
       " 661375.3125,\n",
       " 306019.65625,\n",
       " 491578.65625,\n",
       " 281856.75,\n",
       " 366169.875,\n",
       " 209639.1875,\n",
       " 86294.125,\n",
       " 187934.40625,\n",
       " 966231.625,\n",
       " 244186.5,\n",
       " 273306.40625,\n",
       " 726352.625,\n",
       " 193944.578125,\n",
       " 508340.84375,\n",
       " 656505.0,\n",
       " 156285.4375,\n",
       " 181808.15625,\n",
       " 57287.08203125,\n",
       " 107494.2734375,\n",
       " 144172.0625,\n",
       " 131629.625,\n",
       " 263014.96875,\n",
       " 109199.2265625,\n",
       " 615238.0,\n",
       " 198292.171875,\n",
       " 200738.015625,\n",
       " 418705.9375,\n",
       " 278780.71875,\n",
       " 631334.0,\n",
       " 368017.15625,\n",
       " 434628.875,\n",
       " 206437.171875,\n",
       " 163178.4375,\n",
       " 150369.296875,\n",
       " 242576.734375,\n",
       " 484169.15625,\n",
       " 847300.0625,\n",
       " 329186.65625,\n",
       " 280649.0,\n",
       " 156938.0625,\n",
       " 473921.09375,\n",
       " 290784.125,\n",
       " 86551.6015625,\n",
       " 219572.765625,\n",
       " 529251.1875,\n",
       " 248001.1875,\n",
       " 233987.515625,\n",
       " 606065.25,\n",
       " 69950.8125,\n",
       " 558473.75,\n",
       " 169979.6875,\n",
       " 147208.890625,\n",
       " 317566.40625,\n",
       " 459414.25,\n",
       " 166477.703125,\n",
       " 231013.703125,\n",
       " 272887.125,\n",
       " 421423.78125,\n",
       " 616479.6875,\n",
       " 269948.21875,\n",
       " 210049.25,\n",
       " 175257.515625,\n",
       " 262977.125,\n",
       " 276150.84375,\n",
       " 93635.75,\n",
       " 312994.15625,\n",
       " 344274.78125,\n",
       " 208008.96875,\n",
       " 301575.9375,\n",
       " 117219.4140625,\n",
       " 219714.59375,\n",
       " 367229.4375,\n",
       " 945603.25,\n",
       " 575272.0,\n",
       " 574931.125,\n",
       " 139037.96875,\n",
       " 49549.33984375,\n",
       " 82550.46875,\n",
       " 206820.765625]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(1, 501)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# range of X (no. of epochs)\n",
    "epochs = range(1, len(acc) + 1)\n",
    "epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds = model.predict(X) # see how the model did!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(preds[0]) # i'm spreading that prediction across three nodes and they sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(preds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[215,   0,   0,   0,   0],\n",
       "       [220,   0,   0,   0,   0],\n",
       "       [420,   0,   0,   0,   0],\n",
       "       [ 55,   0,   0,   0,   0],\n",
       "       [111,   0,   0,   0,   0]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Almost a perfect prediction\n",
    "# actual is left, predicted is top\n",
    "# names can be found by inspecting Y\n",
    "matrix = confusion_matrix(dummy_y.argmax(axis=1), preds.argmax(axis=1))\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      1.00      0.35       215\n",
      "           1       0.00      0.00      0.00       220\n",
      "           2       0.00      0.00      0.00       420\n",
      "           3       0.00      0.00      0.00        55\n",
      "           4       0.00      0.00      0.00       111\n",
      "\n",
      "    accuracy                           0.21      1021\n",
      "   macro avg       0.04      0.20      0.07      1021\n",
      "weighted avg       0.04      0.21      0.07      1021\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m099875\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\m099875\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\m099875\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dummy_y.argmax(axis=1), preds.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine \"very\" categories? 19.3.3"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa643c3871e036b0eb6f679a9dba4097931b10046ffb48b434ec4b6bb7e19eba"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
